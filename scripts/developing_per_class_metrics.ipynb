{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s — %(name)s — %(levelname)s — %(funcName)s:%(lineno)d — %(message)s\",\n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect metrics of several volumes using SMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets assume we have multilabel prediction for 3 classes\n",
    "output = np.random.randint(0,3, size=(2,32,32,32))\n",
    "target = np.random.randint(0,3, size=(2,32,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tc=torch.from_numpy(output).long()\n",
    "target_tc=torch.from_numpy(target).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first compute statistics for true positives, false positives, false negative and\n",
    "# true negative \"pixels\"\n",
    "# mode='multiclass' shape (N,C, ...) and torch.LongTensor\n",
    "# mode='multilabel' shape (N, ...) and torch.LongTensor\n",
    "tp, fp, fn, tn = metrics.get_stats(output_tc, target_tc, mode='multilabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 999, 1004, 1099,  963, 1053, 1025,  969, 1024, 1021, 1010, 1022, 1164,\n",
       "         1043, 1026, 1042, 1029, 1041, 1041, 1104, 1001,  985, 1037, 1007, 1024,\n",
       "          967, 1057, 1104, 1084, 1018, 1060, 1029, 1043],\n",
       "        [1032,  984, 1014,  942,  978, 1025,  966, 1004, 1010, 1017, 1048,  924,\n",
       "          992, 1028, 1092,  934, 1063, 1011, 1008, 1033,  994,  962, 1047, 1017,\n",
       "         1101, 1042, 1097, 1060, 1053,  969, 1002, 1068]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 999, 1004, 1099,  963, 1053, 1025,  969, 1024, 1021, 1010, 1022, 1164,\n",
      "         1043, 1026, 1042, 1029, 1041, 1041, 1104, 1001,  985, 1037, 1007, 1024,\n",
      "          967, 1057, 1104, 1084, 1018, 1060, 1029, 1043],\n",
      "        [1032,  984, 1014,  942,  978, 1025,  966, 1004, 1010, 1017, 1048,  924,\n",
      "          992, 1028, 1092,  934, 1063, 1011, 1008, 1033,  994,  962, 1047, 1017,\n",
      "         1101, 1042, 1097, 1060, 1053,  969, 1002, 1068]]) tensor([[   2,   14,  -24,   51,  -38,    5,   43,   28,    4,   12,  -19, -119,\n",
      "          -13,   -4,    2,  -52,   15,   -5,  -45,   39,   40,   12,   26,    6,\n",
      "           39,  -24,  -47,  -54,   10,  -63,  -20,   31],\n",
      "        [ -12,  -12,    4,   39,   45,    2,   47,   55,   21,  -16,   10,   53,\n",
      "           26,   -4,  -68,   55,  -43,   19,   39,    4,    0,   39,    7,   18,\n",
      "          -59,   18,  -34,  -19,  -35,   28,    7,  -37]]) tensor([[ 22, -21, -78,   6,   4, -14,   1,  -4,  -8,  10,   7, -66,   0, -21,\n",
      "         -31,  47, -17, -43, -55, -15,  27,   0, -21,   8,  33, -41, -41, -37,\n",
      "          15,  11,  21, -33],\n",
      "        [ -8,  41, -10,  44,  47,  -7,  38,  19, -12,  14, -32, 101,  41,  22,\n",
      "         -55,  60, -20,  -9,   5, -17,  45,  33, -18,  -6, -18, -12, -67,  -4,\n",
      "          -8,  12,   6,  14]]) tensor([[  1,  27,  27,   4,   5,   8,  11, -24,   7,  -8,  14,  45,  -6,  23,\n",
      "          11,   0, -15,  31,  20,  -1, -28, -25,  12, -14, -15,  32,   8,  31,\n",
      "         -19,  16,  -6, -17],\n",
      "        [ 12,  11,  16,  -1, -46,   4, -27, -54,   5,   9,  -2, -54, -35, -22,\n",
      "          55, -25,  24,   3, -28,   4, -15, -10, -12,  -5,   0, -24,  28, -13,\n",
      "          14,  15,   9, -21]])\n"
     ]
    }
   ],
   "source": [
    "print(tp,fp,fn,tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0011)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then compute metrics with required reduction (see metric docs)\n",
    "#iou_score = metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "iou_score = metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0007)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9765, 1.0070, 1.1023, 0.9441, 1.0334, 1.0089, 0.9566, 0.9771, 1.0039,\n",
       "         0.9787, 1.0119, 1.1890, 1.0126, 1.0250, 1.0286, 1.0049, 1.0019, 1.0483,\n",
       "         1.0996, 0.9766, 0.9363, 0.9886, 0.9951, 0.9865, 0.9307, 1.0655, 1.0866,\n",
       "         1.0916, 0.9760, 1.0516, 0.9990, 1.0019],\n",
       "        [1.0198, 0.9714, 1.0060, 0.9190, 0.9140, 1.0049, 0.9191, 0.9314, 0.9912,\n",
       "         1.0020, 1.0214, 0.8571, 0.9367, 0.9828, 1.1269, 0.8904, 1.0630, 0.9902,\n",
       "         0.9582, 1.0127, 0.9567, 0.9304, 1.0106, 0.9883, 1.0752, 0.9943, 1.1014,\n",
       "         1.0222, 1.0426, 0.9604, 0.9872, 1.0220]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.iou_score(tp, fp, fn, tn, reduction=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0002)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score = metrics.f1_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9948)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not appropriate to what I wish to do. There is no option to seperate metrics for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_stats_per_class(data_lbls,target, nclasses=None):\n",
    "    \"\"\"\n",
    "    Gets true positive, false positive, false negative and true negative sums (tp,fp,fn,tn)\n",
    "    for each class\n",
    "\n",
    "    If nclasses is None (default) then it gets for all the classes based in the maximum\n",
    "    value of both data_lbls and target\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(data_lbls, np.ndarray):\n",
    "        data_lbls = torch.from_numpy(data_lbls)\n",
    "\n",
    "    if isinstance(target, np.ndarray):\n",
    "        target = torch.from_numpy(target)\n",
    "    \n",
    "    data_flat = torch.ravel(data_lbls)\n",
    "    target_flat = torch.ravel(target)\n",
    "    \n",
    "    #Get max class\n",
    "    if nclasses is None:\n",
    "        nclasses = max(int(torch.max(data_flat)), int(torch.max(target_flat)) )+1\n",
    "        logging.info(f\"nclasses:{nclasses}\")\n",
    "\n",
    "    stats_per_class=[]\n",
    "    for iclass in range(nclasses):\n",
    "        pred_bin = data_flat==iclass\n",
    "        gnd_bin =  target_flat==iclass\n",
    "        tp = int(torch.sum(torch.bitwise_and(pred_bin,gnd_bin)))\n",
    "        tn = int(torch.sum(torch.bitwise_and(torch.bitwise_not(pred_bin),torch.bitwise_not(gnd_bin))))\n",
    "        fp = int(torch.sum(torch.bitwise_and(pred_bin,torch.bitwise_not(gnd_bin))))\n",
    "        fn = int(torch.sum(torch.bitwise_and(torch.bitwise_not(pred_bin),gnd_bin)))\n",
    "        metr_t= (tp,fp,fn,tn)  # Same order as used in SMP get_stats\n",
    "        stats_per_class.append(metr_t)\n",
    "\n",
    "    return stats_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_stats(tp,fp,fn,tn):\n",
    "    return (tp+tn)/(tp+tn+fp+fn)\n",
    "\n",
    "def iou_from_stats(tp,fp,fn,tn):\n",
    "    return tp/(tp+fp+fn)\n",
    "\n",
    "def f1dice_from_stats(tp,fp,fn,tn):\n",
    "    return 2*tp/(2*tp+fp+fn)\n",
    "\n",
    "def recall_from_stats(tp,fp,fn,tn):\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def precision_from_stats(tp,fp,fn,tn):\n",
    "    return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 14:11:14,977 — root — INFO — get_metric_stats_per_class:22 — nclasses:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7255, 14528, 14511, 29242),\n",
       " (7386, 14459, 14637, 29054),\n",
       " (7257, 14651, 14490, 29138)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = get_metric_stats_per_class(output, target)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:0, f1dice = 0.3331879032813612, accuracy=0.5569000244140625\n",
      "class:1, f1dice = 0.3367374851828212, accuracy=0.5560302734375\n",
      "class:2, f1dice = 0.3324705073874699, accuracy=0.5553436279296875\n"
     ]
    }
   ],
   "source": [
    "for i,st0 in enumerate(stats):\n",
    "    print(f\"class:{i}, f1dice = {f1dice_from_stats(*stats[i])}, accuracy={accuracy_from_stats(*stats[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
