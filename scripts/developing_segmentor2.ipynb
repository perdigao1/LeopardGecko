{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop segmentor2 with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "#import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "import tempfile\n",
    "import logging\n",
    "from types import SimpleNamespace\n",
    "import tqdm #progress bar in iterations\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import albumentations as alb\n",
    "import albumentations.pytorch\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vol_norm_process = \"mean_stdev_3\" #standard clipping\n",
    "\n",
    "cuda_device=0\n",
    "\n",
    "nn1_loss_criterion='DiceLoss'\n",
    "nn1_eval_metric='MeanIoU'\n",
    "nn1_lr=1e-5\n",
    "nn1_max_lr=3e-2\n",
    "\n",
    "# nn1_epochs = 15\n",
    "nn1_epochs = 5 # debug\n",
    "\n",
    "nn1_batch_size = 2\n",
    "nn1_num_workers = 1\n",
    "\n",
    "#Default\n",
    "nn1_models_class_generator = [{\n",
    "'class':'smp', #smp: segmentation models pytorch\n",
    "'arch': 'U_Net',\n",
    "'encoder_name': 'resnet34',\n",
    "'encoder_weights': 'imagenet', # TODO: support for using existing models (loading)\n",
    "'in_nchannels':1,\n",
    "'nclasses':3,\n",
    "}]\n",
    "\n",
    "nn1_axes_to_models_indices = [0,1,2] # By default use the same model for all axes\n",
    "# To use different models, use [0,1,2] for model0 along z, model1 along y, and model2 along x\n",
    "\n",
    "temp_data_outdir = None\n",
    "cuda_str = f\"cuda:{cuda_device}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segm2 = lgs2.cMultiAxisRotationsSegmentor2.create_simple_separate_models_per_axis(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainlabels max value is 2, so 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses =3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup NN1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_dict_gen = {'class':'smp', #smp: segmentation models pytorch\n",
    "    'arch': 'U_Net',\n",
    "    'encoder_name': 'resnet34',\n",
    "    'encoder_weights': 'imagenet', # TODO: support for using existing models (loading)\n",
    "    'in_nchannels':1, #greyscale\n",
    "    'nclasses':nclasses,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_models_class_generator = [nn1_dict_gen,\n",
    "    nn1_dict_gen.copy(),\n",
    "    nn1_dict_gen.copy()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_models_class_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn1_ptmodel_from_class_generator(nn1_cls_gen_dict: dict):\n",
    "    # get segm model from dictionary item\n",
    "    model0=None\n",
    "\n",
    "    if nn1_cls_gen_dict['class'].lower()=='smp': #unet, AttentionNet (manet) and fpn\n",
    "        #Segmentation models pytorch\n",
    "        arch = nn1_cls_gen_dict['arch'].lower()\n",
    "        if arch==\"unet\" or arch==\"u_net\":\n",
    "            NN_class = smp.Unet\n",
    "        elif arch==\"manet\":\n",
    "            model0 = smp.MAnet\n",
    "        elif arch==\"fpn\":\n",
    "            model0 = smp.FPN\n",
    "        else:\n",
    "            raise ValueError(f\"arch:{arch} not valid.\")\n",
    "        \n",
    "        model0 = NN_class(\n",
    "            encoder_name = nn1_cls_gen_dict['encoder_name'],\n",
    "            encoder_weights = nn1_cls_gen_dict['encoder_weights'],\n",
    "            in_channels = nn1_cls_gen_dict['in_nchannels'],\n",
    "            classes = nn1_cls_gen_dict['nclasses'],\n",
    "            #activation = \"sigmoid\" # Whether to use activation or not, depends whether the loss function require slogits or not\n",
    "            activation = None\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"class {nn1_cls_gen_dict['class']} not supported.\")\n",
    "    \n",
    "    # TODO: add other 2D model support, not just SMPs\n",
    "\n",
    "    return model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN1_models = [ create_nn1_ptmodel_from_class_generator(x).to(f\"cuda:{cuda_device}\") for x in nn1_models_class_generator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN1_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(NN1_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_axes_to_models_indices = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_models = np.unique(nn1_axes_to_models_indices)\n",
    "idx_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and create dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels_fn=[\n",
    "    (\"./test_data/TS_0005_crop.tif\", \"./test_data/TS_0005_ribos_membr_crop.tif\"),\n",
    "]\n",
    "\n",
    "traindatas=[]\n",
    "trainlabels=[]\n",
    "\n",
    "for datafn0, labelfn0 in data_labels_fn:\n",
    "     #Make sure data and labels are curated in the correct data format\n",
    "    traindatas.append(tifffile.imread(datafn0))\n",
    "    trainlabels.append(tifffile.imread(labelfn0)) #In this case labels are already in uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainlabels[0].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise data to \"mean_stdev_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_list0=[]\n",
    "\n",
    "# Clip data to -3*stdev and +3*stdev and normalises to values between 0 and 1\n",
    "for d0 in traindatas:\n",
    "    d0_mean = np.mean(d0)\n",
    "    d0_std = np.std(d0)\n",
    "\n",
    "    if d0_std==0:\n",
    "        raise ValueError(\"Error. Stdev of data volume is zero.\")\n",
    "    \n",
    "    d0_corr = (d0.astype(np.float32) - d0_mean) / d0_std\n",
    "    d0_corr = (np.clip(d0_corr, -3.0, 3.0) +3.0) / 6.0\n",
    "    \n",
    "    traindata_list0.append(d0_corr*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_list = [ t.astype(np.uint8) for t in traindata_list0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traindata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_list[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some slices to ensure the data is properly loaded\n",
    "import random\n",
    "randomints= np.random.default_rng().permutation(256)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(10,5))\n",
    "#fig.tight_layout()\n",
    "for i in range(4):\n",
    "    ir = randomints[i]\n",
    "    axs[i].imshow(traindata_list[0][ir,:,:], cmap=\"gray\", vmin=0, vmax=255)\n",
    "    axs[i].set_axis_off()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets and dataloaders for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_augmentations_v0(h,w):\n",
    "\n",
    "    def get_nearest_multiple_of_32(v):\n",
    "        i32 = v//32\n",
    "        return i32*32\n",
    "\n",
    "    img_h, img_w = h,w\n",
    "\n",
    "    img_h32, img_w32 = get_nearest_multiple_of_32(img_h),  get_nearest_multiple_of_32(img_w)\n",
    "    assert img_h32>0 and img_w>0\n",
    "\n",
    "    tfms0 =alb.Compose(\n",
    "                [\n",
    "                alb.RandomSizedCrop(\n",
    "                    min_max_height= (img_h32//2, img_h32),\n",
    "                    height=img_h32,\n",
    "                    width=img_w32 ,\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                #Deciding what resizing augmentations is difficult not kowing what\n",
    "                # sizes the images can be different\n",
    "\n",
    "                alb.VerticalFlip(p=0.5),\n",
    "                alb.RandomRotate90(p=0.5),\n",
    "                alb.Transpose(p=0.5),\n",
    "                alb.OneOf(\n",
    "                    [\n",
    "                        alb.ElasticTransform(\n",
    "                            alpha=120, sigma=120 * 0.07, alpha_affine=120 * 0.04, p=0.5\n",
    "                        ),\n",
    "                        alb.GridDistortion(p=0.5),\n",
    "                        alb.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=0.5),\n",
    "                    ],\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                alb.CLAHE(p=0.5),\n",
    "                alb.OneOf([alb.RandomBrightnessContrast(p=0.5),alb.RandomGamma(p=0.5)], p=0.5),\n",
    "                alb.pytorch.ToTensorV2()\n",
    "                ]\n",
    "            )\n",
    "    return tfms0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check\n",
    "class NN1_train_input_dataset_along_axes(Dataset):\n",
    "    def __init__(self, datavols_list, labelsvols_list, axes=[0,1,2], cuda_device_str=0):\n",
    "        \n",
    "        self.cuda_device_str = cuda_device_str\n",
    "        self.datavols_list = datavols_list\n",
    "        self.labelsvols_list = labelsvols_list\n",
    "        self.axes = axes\n",
    "\n",
    "        #given an idx number, retrive the item, axis, slice number and transform\n",
    "        self._idx_to_item=[]\n",
    "        self._idx_to_ax=[]\n",
    "        self._idx_to_slicen=[]\n",
    "        self._idx_to_tfms = []\n",
    "\n",
    "        #total_slices=0\n",
    "        for id, d0 in enumerate(datavols_list):\n",
    "            for ia, ax0 in enumerate(axes):\n",
    "                nslices=d0.shape[ax0]\n",
    "                #total_slices+= nslices\n",
    "\n",
    "                id0_to_item = [id]*nslices\n",
    "                self._idx_to_item.extend(id0_to_item)\n",
    "\n",
    "                ax0_to_item = [ax0]*nslices\n",
    "                self._idx_to_ax.extend(ax0_to_item)\n",
    "\n",
    "                slice_range = np.arange(0,nslices).tolist()\n",
    "                self._idx_to_slicen.extend(slice_range)\n",
    "\n",
    "                if ax0==0:\n",
    "                    t0 = get_train_augmentations_v0( *d0[0,:,:].shape )\n",
    "                elif ax0==1:\n",
    "                    t0 = get_train_augmentations_v0( *d0[:,0,:].shape )\n",
    "                elif ax0==2:\n",
    "                    t0 = get_train_augmentations_v0( *d0[:,:,0].shape )\n",
    "                else:\n",
    "                    raise ValueError(f\"ax0 {ax0} not valid\")\n",
    "                self._idx_to_tfms.extend([t0]*nslices)\n",
    "\n",
    "        total_slices = len(self._idx_to_item)\n",
    "\n",
    "        assert total_slices==len(self._idx_to_ax) and total_slices==len(self._idx_to_ax) and total_slices==len(self._idx_to_slicen) and total_slices==len(self._idx_to_tfms)\n",
    "\n",
    "        self.len = total_slices\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        it = self._idx_to_item[idx]\n",
    "        ax = self._idx_to_ax[idx]\n",
    "        slicen = self._idx_to_slicen[idx]\n",
    "        tfms = self._idx_to_tfms[idx]\n",
    "        \n",
    "        if ax==0:\n",
    "            data_slice = self.datavols_list[it][slicen,:,:]\n",
    "            labels_slice = self.labelsvols_list[it][slicen,:,:]\n",
    "        elif ax==1:\n",
    "            data_slice = self.datavols_list[it][:,slicen,:]\n",
    "            labels_slice = self.labelsvols_list[it][:,slicen,:]\n",
    "        elif ax==2:\n",
    "            data_slice = self.datavols_list[it][:,:,slicen]\n",
    "            labels_slice = self.labelsvols_list[it][:,:,slicen]\n",
    "        else:\n",
    "            raise ValueError(f\"ax {ax} not valid\")\n",
    "\n",
    "        assert data_slice.shape == labels_slice.shape\n",
    "\n",
    "        # Apply transforms\n",
    "        res =tfms(image=data_slice, mask=labels_slice)\n",
    "\n",
    "        data=res['image']\n",
    "        labels=res['mask']\n",
    "        \n",
    "        data= data.to(self.cuda_device_str).float()\n",
    "        labels=labels.to(self.cuda_device_str).long()\n",
    "\n",
    "        #return a tuple data, mask\n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabels_list = trainlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test creating a dataset\n",
    "ds0 = NN1_train_input_dataset_along_axes(\n",
    "    traindata_list,\n",
    "    trainlabels_list,\n",
    "    [1,2], # change axis as desired\n",
    "    cuda_device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds0._idx_to_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise data and respective labels from datasets (no transforms applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages=5\n",
    "fig,axs = plt.subplots(1,nimages,figsize=(15,5))\n",
    "\n",
    "randomints= np.random.default_rng().permutation(256)\n",
    "\n",
    "for i in range(nimages):\n",
    "    r0 = randomints[i]\n",
    "    datai, labeli = ds0[r0]\n",
    "    print(f\"i:{i}, datai shape:{datai.shape}, type:{datai.dtype}   label shape:{labeli.shape}, type:{labeli.dtype}\")\n",
    "    datai=datai.detach().cpu().numpy()[0,:,:]\n",
    "    labeli = labeli.detach().cpu().numpy()[:,:]\n",
    "    axs[i].imshow(datai, cmap=\"gray\")\n",
    "    axs[i].imshow(labeli,cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[i].set_axis_off()\n",
    "\n",
    "    if i==nimages-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_axes_to_models_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn1_axes_to_models_indices = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flatnonzero(\n",
    "        np.array(nn1_axes_to_models_indices) == 2\n",
    "    ).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_train=[]\n",
    "dataloaders_test=[]\n",
    "for i in range(len(NN1_models)):\n",
    "    #Gets the axes that the NN1 model is supposed to be used\n",
    "    model_axes= np.flatnonzero(\n",
    "        np.array(nn1_axes_to_models_indices) == i\n",
    "    ).tolist()\n",
    "\n",
    "    dl_train=None\n",
    "    dl_test=None\n",
    "\n",
    "    if len(model_axes)>0:\n",
    "\n",
    "        ds0 = NN1_train_input_dataset_along_axes(\n",
    "            traindata_list,\n",
    "            trainlabels_list,\n",
    "            model_axes,\n",
    "            cuda_device\n",
    "        )\n",
    "\n",
    "        dset1, dset2 = torch.utils.data.random_split(ds0, [0.8,0.2])\n",
    "\n",
    "        dl_train = DataLoader(dset1, batch_size=nn1_batch_size, shuffle=True)\n",
    "        dl_test = DataLoader(dset2, batch_size=nn1_batch_size, shuffle=True)\n",
    "\n",
    "    dataloaders_train.append(dl_train)\n",
    "    dataloaders_test.append(dl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualise some data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages=5\n",
    "fig,axs = plt.subplots(1,nimages,figsize=(15,5))\n",
    "for i, (datai,labeli) in enumerate(dataloaders_train[2]): # Change index to 0,1,2 for z,y,x\n",
    "    print(f\"i:{i}, datai shape:{datai.shape}, type:{datai.dtype}   label shape:{labeli.shape}, type:{labeli.dtype}\")\n",
    "    datai=datai.detach().cpu().numpy()[0,0,:,:]\n",
    "    labeli = labeli.detach().cpu().numpy()[0,:,:]\n",
    "    axs[i].imshow(datai, cmap=\"gray\")\n",
    "    axs[i].imshow(labeli,cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[i].set_axis_off()\n",
    "\n",
    "    if i==nimages-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_loss_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_loss_func_and_activ = None\n",
    "activ = torch.nn.Sigmoid()\n",
    "if \"crossentropyloss\" in nn1_loss_criterion.lower():\n",
    "    nn1_loss_func = torch.nn.CrossEntropyLoss().to(cuda_str) # expects logits!\n",
    "    \n",
    "    # or can use\n",
    "    # nn1_loss_func = torch.nn.functional.cross_entropy(pred_logits, target)\n",
    "    \n",
    "    nn1_loss_func_and_activ= {\"func\":nn1_loss_func, \"activ\":activ}\n",
    "elif \"diceloss\" in nn1_loss_criterion.lower():\n",
    "    nn1_loss_func = smp.losses.DiceLoss(mode='multiclass', from_logits=True).to(cuda_str)\n",
    "    nn1_loss_func_and_activ= {\"func\":nn1_loss_func, \"activ\":None}\n",
    "else:\n",
    "    raise ValueError(f\"{nn1_loss_criterion} not a valid loss criteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_loss_func_and_activ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup metrics for test data\n",
    "nn1_metric_func = None\n",
    "if \"iou\" in nn1_eval_metric.lower():\n",
    "    nn1_metric_func = segmentation_models_pytorch.utils.metrics.IoU()\n",
    "elif \"dice\" in nn1_eval_metric.lower() or \"fscore\" in nn1_eval_metric.lower():\n",
    "    nn1_metric_func = segmentation_models_pytorch.utils.metrics.Fscore()\n",
    "elif \"accuracy\" in nn1_eval_metric.lower():\n",
    "    nn1_metric_func = segmentation_models_pytorch.utils.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_metric_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training of each model individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_func_and_activ, optimizer, scaler, scheduler, do_log=True):\n",
    "    loss_fn = loss_func_and_activ[\"func\"]\n",
    "    activ_fn = loss_func_and_activ[\"activ\"]\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        #X=X_parse(X)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "\n",
    "        if activ_fn is not None:\n",
    "            pred = activ_fn(pred)\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        #loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        #optimizer.step() #step done by the scheduler\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if do_log and batch % 50 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            logging.info(f\"batch:{batch}  loss: {loss:>7f}  [{current:>5d}/{size:>5d}]. lr:{scheduler.get_last_lr()}\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_func_and_activ, metric_fn=None):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    #size = len(dataloader.dataset)\n",
    "    #num_batches = len(dataloader)\n",
    "\n",
    "    loss_fn = loss_func_and_activ[\"func\"]\n",
    "    activ_fn = loss_func_and_activ[\"activ\"]\n",
    "\n",
    "    test_losses=[]\n",
    "    test_metrics=[]\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            #X=X_parse(X)\n",
    "            #y=y_parse(y)\n",
    "            pred = model(X)\n",
    "\n",
    "            if activ_fn is not None:\n",
    "                pred = activ_fn(pred)\n",
    "\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            test_loss = loss.item()\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            if metric_fn is not None:\n",
    "                pred_argmax = torch.argmax(pred, dim=1)\n",
    "                metric = metric_fn(pred_argmax, y).item()\n",
    "                test_metrics.append(metric)\n",
    "            # #metric\n",
    "            # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    avg_loss = np.mean(np.array(test_losses))\n",
    "    logging.info(f\"Avg loss: {avg_loss:>8f}\")\n",
    "\n",
    "    avg_metric=None\n",
    "    if not metric_fn is None:\n",
    "        avg_metric = np.mean(np.array(test_metrics))\n",
    "        logging.info(f\"Avg metric: {avg_metric:>8f}\")\n",
    "\n",
    "    return {\"avg_loss\":avg_loss, \"avg_metric\":avg_metric, \"test_metrics\":test_metrics, \"test_losses\":test_losses}\n",
    "\n",
    "def train_model(model0, dl_train, dl_test, loss_func_and_activ, optimizer, scaler, scheduler, epochs, metric_fn=None):\n",
    "    logging.info(\"train_model()\")\n",
    "    test_results=[]\n",
    "    for t in range(epochs):\n",
    "        logging.info(f\"---- Epoch {t+1}/{epochs} ----\")\n",
    "        train_loop(dl_train, model0, loss_func_and_activ, optimizer, scaler, scheduler)\n",
    "\n",
    "        test_res=None\n",
    "        if dl_test is not None:\n",
    "            test_res= test_loop(dl_test, model0, loss_func_and_activ, metric_fn=metric_fn)\n",
    "            test_results.append(test_res)\n",
    "    logging.info(f\"Done!\")\n",
    "    if dl_test is not None:\n",
    "        logging.info(f\"Final test loss is : {test_res['avg_loss']}, and metric is: {test_res['avg_metric']}\")\n",
    "    return {\"test_results\": test_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(NN1_models))\n",
    "print(len(dataloaders_train))\n",
    "print(len(dataloaders_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= NN1_models[0]\n",
    "dl_train0 = dataloaders_train[0]\n",
    "dl_test0 = dataloaders_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup optimizer and scaler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=nn1_lr)\n",
    "scaler=torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = nn1_epochs\n",
    "#epochs = 10\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr= nn1_max_lr,\n",
    "    steps_per_epoch=len(dl_train0),\n",
    "    epochs=epochs,\n",
    "    #pct_start=0.1, #default=0.3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, dl_train0, dl_test0, nn1_loss_func_and_activ, optimizer, scaler, scheduler,\n",
    "            epochs=epochs,\n",
    "            metric_fn=nn1_metric_func\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of some slices of first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= NN1_models[0]\n",
    "model.eval()\n",
    "nimages=3\n",
    "fig,axs = plt.subplots(2,nimages,figsize=(15,10))\n",
    "for i, (datai,labeli) in enumerate(dataloaders_train[0]):\n",
    "    print(f\"i:{i}, datai shape:{datai.shape}, type:{datai.dtype}   label shape:{labeli.shape}, type:{labeli.dtype}\")\n",
    "    #datai=datai.detach().cpu().numpy()[0,0,:,:]\n",
    "    labeli = labeli.detach().cpu().numpy()[0,:,:]\n",
    "    \n",
    "    X=datai\n",
    "    pred=model(X)\n",
    "    pred_argmax = torch.argmax(pred, dim=1)\n",
    "\n",
    "    pred_np = pred_argmax.detach().cpu().numpy()[0,:,:]\n",
    "    print(f\"i:{i}, pred_max:{pred_np.max()}\")\n",
    "    datai_np = datai.detach().cpu().numpy()[0,0,:,:]\n",
    "    axs[0,i].imshow(datai_np, cmap=\"gray\")\n",
    "    axs[0,i].imshow(pred_np,cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[0,i].set_axis_off()\n",
    "\n",
    "    axs[1,i].imshow(datai_np, cmap=\"gray\")\n",
    "    axs[1,i].imshow(labeli,cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[1,i].set_axis_off()\n",
    "\n",
    "    if i==nimages-1:\n",
    "        break\n",
    "\n",
    "    #predictions on top, ground truth at the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model (Y axis) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= NN1_models[1]\n",
    "dl_train0 = dataloaders_train[1]\n",
    "dl_test0 = dataloaders_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup optimizer and scaler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=nn1_lr)\n",
    "scaler=torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = nn1_epochs\n",
    "#epochs = 10\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr= nn1_max_lr,\n",
    "    steps_per_epoch=len(dl_train0),\n",
    "    epochs=epochs,\n",
    "    #pct_start=0.1, #default=0.3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, dl_train0, dl_test0, nn1_loss_func_and_activ, optimizer, scaler, scheduler,\n",
    "            epochs=epochs,\n",
    "            metric_fn=nn1_metric_func\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of some slices of second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= NN1_models[1]\n",
    "model.eval()\n",
    "nimages=3\n",
    "fig,axs = plt.subplots(2,nimages,figsize=(15,10))\n",
    "for i, (datai,labeli) in enumerate(dataloaders_train[1]):\n",
    "    print(f\"i:{i}, datai shape:{datai.shape}, type:{datai.dtype}   label shape:{labeli.shape}, type:{labeli.dtype}\")\n",
    "    #datai=datai.detach().cpu().numpy()[0,0,:,:]\n",
    "    labeli = labeli.detach().cpu().numpy()[0,:,:]\n",
    "    \n",
    "    X=datai\n",
    "    pred=model(X)\n",
    "    pred_argmax = torch.argmax(pred, dim=1)\n",
    "\n",
    "    pred_np = pred_argmax.detach().cpu().numpy()[0,:,:]\n",
    "    print(f\"i:{i}, pred_max:{pred_np.max()}\")\n",
    "    datai_np = datai.detach().cpu().numpy()[0,0,:,:]\n",
    "    axs[0,i].imshow(datai_np, cmap=\"gray\")\n",
    "    axs[0,i].imshow(pred_np,cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[0,i].set_axis_off()\n",
    "\n",
    "    axs[1,i].imshow(datai_np, cmap=\"gray\")\n",
    "    axs[1,i].imshow(labeli,cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[1,i].set_axis_off()\n",
    "\n",
    "    if i==nimages-1:\n",
    "        break\n",
    "\n",
    "    #predictions on top, ground truth at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third model (X axis) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= NN1_models[2]\n",
    "dl_train0 = dataloaders_train[2]\n",
    "dl_test0 = dataloaders_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup optimizer and scaler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=nn1_lr)\n",
    "scaler=torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = nn1_epochs\n",
    "#epochs = 10\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr= nn1_max_lr,\n",
    "    steps_per_epoch=len(dl_train0),\n",
    "    epochs=epochs,\n",
    "    #pct_start=0.1, #default=0.3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, dl_train0, dl_test0, nn1_loss_func_and_activ, optimizer, scaler, scheduler,\n",
    "            epochs=epochs,\n",
    "            metric_fn=nn1_metric_func\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of some slices of second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= NN1_models[2]\n",
    "model.eval()\n",
    "nimages=3\n",
    "fig,axs = plt.subplots(2,nimages,figsize=(15,10))\n",
    "for i, (datai,labeli) in enumerate(dataloaders_train[2]):\n",
    "    print(f\"i:{i}, datai shape:{datai.shape}, type:{datai.dtype}   label shape:{labeli.shape}, type:{labeli.dtype}\")\n",
    "    #datai=datai.detach().cpu().numpy()[0,0,:,:]\n",
    "    labeli = labeli.detach().cpu().numpy()[0,:,:]\n",
    "    \n",
    "    X=datai\n",
    "    pred=model(X)\n",
    "    pred_argmax = torch.argmax(pred, dim=1)\n",
    "\n",
    "    pred_np = pred_argmax.detach().cpu().numpy()[0,:,:]\n",
    "    print(f\"i:{i}, pred_max:{pred_np.max()}\")\n",
    "    datai_np = datai.detach().cpu().numpy()[0,0,:,:]\n",
    "    axs[0,i].imshow(datai_np, cmap=\"gray\")\n",
    "    axs[0,i].imshow(pred_np,cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[0,i].set_axis_off()\n",
    "\n",
    "    axs[1,i].imshow(datai_np, cmap=\"gray\")\n",
    "    axs[1,i].imshow(labeli,cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[1,i].set_axis_off()\n",
    "\n",
    "    if i==nimages-1:\n",
    "        break\n",
    "\n",
    "    #predictions on top, ground truth at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict volume(s) using the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup temporary folder to store the predicted volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir_pred= tempfile.TemporaryDirectory()\n",
    "path_out_results = Path(tempdir_pred.name)\n",
    "logging.info(f\"tempdir_pred_path:{path_out_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "def _save_pred_data(folder, data, count,axis, rot):\n",
    "    # Saves predicted data to h5 file in tempdir and return file path in case it is needed\n",
    "    file_path = f\"{folder}/pred_{count}_{axis}_{rot}.h5\"\n",
    "\n",
    "    logging.info(f\"Saving data of shape {data.shape} to {file_path}.\")\n",
    "    with h5py.File(file_path, \"w\") as f:\n",
    "        f.create_dataset(\"/data\", data=data)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_predict = traindata_list[0] #Only first volume for testing.\n",
    "# Volumes in traindata_list has already been normalised/clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumeSlicerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datavol, axis, per_slice_tfms=None, device_str=\"cuda:0\"):\n",
    "        assert datavol.ndim==3\n",
    "        assert axis==0 or axis==1 or axis==2\n",
    "\n",
    "        self.datavol=datavol\n",
    "        self.axis=axis\n",
    "        self.per_slice_tfms=per_slice_tfms\n",
    "        self.device_str = device_str\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datavol.shape[self.axis]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data_slice=None\n",
    "        if self.axis==0:\n",
    "            data_slice = self.datavol[idx,:,:]\n",
    "        elif self.axis==1:\n",
    "            data_slice = self.datavol[:,idx,:]\n",
    "        elif self.axis==2:\n",
    "            data_slice = self.datavol[:,:,idx]\n",
    "\n",
    "        res = data_slice\n",
    "        # Apply transform\n",
    "        if self.per_slice_tfms is not None:\n",
    "            res = self.per_slice_tfms(data_slice)\n",
    "\n",
    "        #Convert to tensor and send to device\n",
    "        res_torch = torch.unsqueeze(torch.from_numpy(res), dim=0).float().to(self.device_str)\n",
    "\n",
    "        return res_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "ds0 = VolumeSlicerDataset(data_to_predict, axis=0 , per_slice_tfms=None, device_str=cuda_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages=5\n",
    "fig,axs = plt.subplots(1,nimages,figsize=(15,5))\n",
    "\n",
    "randomints= np.random.default_rng().permutation(256)\n",
    "\n",
    "for i in range(nimages):\n",
    "    r0 = randomints[i]\n",
    "    datai_t = ds0[r0]\n",
    "    print(f\"i:{i}, datai shape:{datai_t.shape}, dtype:{datai_t.dtype}, type:{type(datai_t)}\")\n",
    "    datai=datai_t.detach().cpu().numpy()[0,:,:]\n",
    "    axs[i].imshow(datai, cmap=\"gray\")\n",
    "    axs[i].set_axis_off()\n",
    "\n",
    "    if i==nimages-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl0 = DataLoader(dataset=ds0, batch_size=nn1_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use model0\n",
    "model=NN1_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "SM_func = torch.nn.Softmax(dim=1) # to get probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run predictions for the whole volume along axes previously specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for ibatch, x in enumerate(dl0):\n",
    "        logging.info(f\"ibatch: {ibatch} \")\n",
    "        X= model(x)\n",
    "        #logging.info(f\"X.shape:{X.shape}\")\n",
    "\n",
    "        pred_probs_slice = SM_func(X)\n",
    "        #logging.info(f\"pred_probs_slice.shape:{pred_probs_slice.shape}\")\n",
    "        #preds_list.append(pred_probs_slice)\n",
    "\n",
    "        # get labels using argmax\n",
    "        lbl_slice = torch.argmax(pred_probs_slice, dim=1)\n",
    "        #labels_list.append(lbl_slice)\n",
    "\n",
    "        # need to move out from device, otherwise it uses too much RAM\n",
    "\n",
    "        pred_probs_slice_np = pred_probs_slice.detach().cpu().numpy()\n",
    "        lbl_slice_np = lbl_slice.detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        preds_list.append(pred_probs_slice_np)\n",
    "        labels_list.append(lbl_slice_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list_conc = np.concatenate(preds_list, axis=0)\n",
    "preds_list_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_z = np.swapaxes(np.concatenate(preds_list, axis=0),0,1)\n",
    "preds_z = np.transpose(preds_list_conc, axes=(1,0,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred_z = np.concatenate(labels_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view some z-slices and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages=4\n",
    "fig,axs = plt.subplots(1,nimages,figsize=(15,5))\n",
    "\n",
    "randomints= np.random.default_rng().permutation(256)\n",
    "\n",
    "for i in range(nimages):\n",
    "    r0 = randomints[i]\n",
    "    datai = data_to_predict[r0,:,:]\n",
    "    labeli = labels_pred_z[r0,:,:]\n",
    "    axs[i].imshow(datai, cmap=\"gray\")\n",
    "    axs[i].set_axis_off()\n",
    "    axs[i].imshow(labeli , cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages=4\n",
    "fig,axs = plt.subplots(1,nimages,figsize=(15,5))\n",
    "\n",
    "randomints= np.random.default_rng().permutation(256)\n",
    "\n",
    "for i in range(nimages):\n",
    "    r0 = randomints[i]\n",
    "    datai = data_to_predict[r0,:,:]\n",
    "    predi = preds_z[2,r0,:,:]\n",
    "    axs[i].imshow(datai, cmap=\"gray\")\n",
    "    axs[i].set_axis_off()\n",
    "    axs[i].imshow(predi, cmap='viridis', alpha=0.4)\n",
    "    axs[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn1_predict_slices_along_axis(datavol, axis, device_str):\n",
    "    ds0 = VolumeSlicerDataset(datavol, axis , per_slice_tfms=None, device_str=device_str)\n",
    "    dl0 = DataLoader(dataset=ds0, batch_size=nn1_batch_size, shuffle=False)\n",
    "\n",
    "    # Get correct model\n",
    "    model_index = nn1_axes_to_models_indices[axis]\n",
    "    model = NN1_models[model_index]\n",
    "    logging.info(f\"axis:{axis}, use model_index: {model_index}\")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    SM_func = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    for ibatch, x in enumerate(dl0):\n",
    "        # x.shape is (batchsize, 1, 256,256) with 256 being the imagesize\n",
    "        X= model(x)\n",
    "        #pred shape is (batchsize, 3, 256, 256)\n",
    "\n",
    "        pred_probs_slice = SM_func(X) #Convert to probabilities\n",
    "\n",
    "        # get labels using argmax\n",
    "        lbl_slice = torch.argmax(pred_probs_slice, dim=1)\n",
    "        #labels_list.append(lbl_slice)\n",
    "\n",
    "        # need to move away from device, otherwise it uses too much VRAM\n",
    "        pred_probs_slice_np = pred_probs_slice.detach().cpu().numpy()\n",
    "        lbl_slice_np = lbl_slice.detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        preds_list.append(pred_probs_slice_np)\n",
    "        labels_list.append(lbl_slice_np)\n",
    "\n",
    "    logging.info(\"Prediction of all slices complete. Now stacking and getting the right orientation.\")\n",
    "    # stack slices\n",
    "    preds_list_conc = np.concatenate(preds_list, axis=0) # shape will be (256,3,256,256)\n",
    "    labels_pred_conc = np.concatenate(labels_list, axis=0)\n",
    "\n",
    "    pred_oriented = None\n",
    "    labels_oriented = None\n",
    "    if axis==0:\n",
    "        pred_oriented = np.transpose(preds_list_conc, axes=(1,0,2,3))\n",
    "        labels_oriented = labels_pred_conc # no need to orient\n",
    "    elif axis==1:\n",
    "        pred_oriented = np.transpose(preds_list_conc, axes=(1,2,0,3))\n",
    "        labels_oriented = np.transpose(labels_pred_conc, axes=(1,0,2))\n",
    "    elif axis==2:\n",
    "        pred_oriented = np.transpose(preds_list_conc, axes=(1,2,3,0))\n",
    "        labels_oriented = np.transpose(labels_pred_conc, axes=(1,2,0))\n",
    "\n",
    "    #with pred_oriented note that class probability is at the start\n",
    "    return pred_oriented, labels_oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "res = nn1_predict_slices_along_axis(data_to_predict, axis=2, device_str=cuda_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import napari\n",
    "# NV=napari.Viewer()\n",
    "# NV.add_image(data_to_predict)\n",
    "# NV.add_labels(res[1])\n",
    "# NV.add_labels(trainlabels_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import napari\n",
    "# NV=napari.Viewer()\n",
    "# NV.add_image(data_to_predict)\n",
    "# NV.add_image(res[0][1,...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several volumes, different rotations and axis and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_probs_filenames=[] #Will store results in files, and keep the filenames as reference\n",
    "pred_data_labels_filenames=[]\n",
    "pred_sets=[]\n",
    "pred_planes=[]\n",
    "pred_rots=[]\n",
    "pred_ipred=[]\n",
    "pred_shapes=[]\n",
    "itag=0\n",
    "iset=0\n",
    "\n",
    "for krot in range(0, 4): #Around axis rotations\n",
    "    rot_angle_degrees = krot * 90\n",
    "    logging.info(f\"Volume to be rotated by {rot_angle_degrees} degrees\")\n",
    "\n",
    "    #Predict 3 axis\n",
    "    #YX, along Z\n",
    "    # planeYX=(1,2)\n",
    "    logging.info(\"Predicting YX slices, along Z\")\n",
    "    data_vol = np.array(np.rot90(data_to_predict,krot, axes=(1,2))) #rotate\n",
    "\n",
    "    prob0,lab0 = nn1_predict_slices_along_axis(data_vol, axis=0, device_str=cuda_str)\n",
    "\n",
    "    #invert rotations before saving\n",
    "    pred_probs = np.rot90(prob0, -krot, axes=(2,3)) \n",
    "    pred_labels = np.rot90(lab0, -krot, axes=(1,2)) #note that class is at start\n",
    "\n",
    "    fn = _save_pred_data(path_out_results,pred_probs, iset, \"YX\", rot_angle_degrees)\n",
    "    pred_data_probs_filenames.append(fn)\n",
    "    fn = _save_pred_data(path_out_results,pred_labels, iset, \"YX_labels\", rot_angle_degrees)\n",
    "    pred_data_labels_filenames.append(fn)\n",
    "    \n",
    "    pred_sets.append(iset)\n",
    "    pred_planes.append(\"YX\")\n",
    "    pred_rots.append(rot_angle_degrees)\n",
    "    pred_ipred.append(itag)\n",
    "    pred_shapes.append(pred_labels.shape)\n",
    "    itag+=1\n",
    "\n",
    "\n",
    "\n",
    "    #ZX\n",
    "    logging.info(\"Predicting ZX slices, along Y\")\n",
    "    #planeZX=(0,2)\n",
    "    data_vol = np.array(np.rot90(data_to_predict,krot, axes=(0,2))) #rotate\n",
    "    prob0,lab0 = nn1_predict_slices_along_axis(data_vol, axis=1, device_str=cuda_str)\n",
    "\n",
    "    pred_probs = np.rot90(prob0, -krot, axes=(1,3)) #invert rotation before saving\n",
    "    pred_labels = np.rot90(lab0, -krot, axes=(0,2))\n",
    "\n",
    "    fn = _save_pred_data(path_out_results,pred_probs, iset, \"ZX\", rot_angle_degrees)\n",
    "    pred_data_probs_filenames.append(fn)\n",
    "    fn = _save_pred_data(path_out_results,pred_labels, iset, \"ZX_labels\", rot_angle_degrees)\n",
    "    pred_data_labels_filenames.append(fn)\n",
    "    \n",
    "    pred_sets.append(iset)\n",
    "    pred_planes.append(\"ZX\")\n",
    "    pred_rots.append(rot_angle_degrees)\n",
    "    pred_ipred.append(itag)\n",
    "    pred_shapes.append(pred_labels.shape)\n",
    "    itag+=1\n",
    "\n",
    "\n",
    "\n",
    "    #ZY\n",
    "    logging.info(\"Predicting ZY slices, along X\")\n",
    "    #planeZY=(0,1)\n",
    "    data_vol = np.array(np.rot90(data_to_predict,krot, axes=(0,1))) #rotate\n",
    "    prob0,lab0 = nn1_predict_slices_along_axis(data_vol, axis=2, device_str=cuda_str)\n",
    "\n",
    "    pred_probs = np.rot90(prob0, -krot, axes=(1,2)) #invert rotation before saving\n",
    "    pred_labels = np.rot90(lab0, -krot, axes=(0,1))\n",
    "    \n",
    "    fn = _save_pred_data(path_out_results,pred_probs, iset, \"ZY\", rot_angle_degrees)\n",
    "    pred_data_probs_filenames.append(fn)\n",
    "    fn = _save_pred_data(path_out_results,pred_labels, iset, \"ZY_labels\", rot_angle_degrees)\n",
    "    pred_data_labels_filenames.append(fn)\n",
    "    \n",
    "    pred_sets.append(iset)\n",
    "    pred_planes.append(\"ZY\")\n",
    "    pred_rots.append(rot_angle_degrees)\n",
    "    pred_ipred.append(itag)\n",
    "    pred_shapes.append(pred_labels.shape)\n",
    "    itag+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved predictions to folder `C:\\Users\\LUIS-W~1\\AppData\\Local\\Temp\\tmp0rbspoqt/ `\n",
    "\n",
    "Collect information to a pandas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_pd = pd.DataFrame({\n",
    "    'pred_data_probs_filenames': pred_data_probs_filenames,\n",
    "    'pred_data_labels_filenames': pred_data_labels_filenames,\n",
    "    'pred_sets':pred_sets,\n",
    "    'pred_planes':pred_planes,\n",
    "    'pred_rots':pred_rots,\n",
    "    'pred_ipred':pred_ipred,\n",
    "    'pred_shapes': pred_shapes,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save pandas table in case we need to exit before training NN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_pd.to_csv(\"developing_segmentor2_nn1_temp_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN2 training\n",
    "\n",
    "Similar to segmentor.py but using own pytorch MLP classifier (Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "#import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "import tempfile\n",
    "import logging\n",
    "from types import SimpleNamespace\n",
    "import tqdm #progress bar in iterations\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as alb\n",
    "import albumentations.pytorch\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2_MLP_models_class_generator = {\n",
    "    \"nn2_hidden_layer_sizes\" : \"10,10\",\n",
    "    \"nn2_activation\": 'tanh',\n",
    "    \"nn2_out_nclasses\": 3,\n",
    "    \"nn2_in_nchannels\": 3*12\n",
    "}\n",
    "\n",
    "#nn2_max_iter = 1000\n",
    "nn2_ntrain = 262144 #Note that this is not a MLPClassifier parameter\n",
    "nn2_train_epochs = 20\n",
    "nn2_batch_size = 4096\n",
    "nn2_lr = 1e-6\n",
    "nn2_max_lr = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load csv file\n",
    "all_pred_pd = pd.read_csv(\"developing_segmentor2_nn1_temp_results.csv\")\n",
    "all_pred_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_pd[\"pred_data_probs_filenames\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar to segmentor.py\n",
    "import h5py\n",
    "data_all_np5d=None\n",
    "\n",
    "logging.debug(\"Aggregating multiple sets onto a single volume data_all_np5d\")\n",
    "# aggregate multiple sets for data\n",
    "for i,prow in all_pred_pd.iterrows():\n",
    "\n",
    "    prob_filename = prow['pred_data_probs_filenames']\n",
    "    with h5py.File(prob_filename,'r') as f:\n",
    "        data0 = np.array(f[\"data\"])\n",
    "\n",
    "    if i==0:\n",
    "        #initialise\n",
    "        logging.info(f\"data0.shape:{data0.shape}\")\n",
    "        all_shape0 = (\n",
    "            1, # needs to be adjusted\n",
    "            12, # needs to be adjusted, perhaps can be collected from dataframe\n",
    "            *data0.shape\n",
    "            )\n",
    "\n",
    "        data_all_np5d=np.zeros( all_shape0 , dtype=data0.dtype)\n",
    "\n",
    "    \n",
    "    ipred=prow['pred_ipred']\n",
    "    iset=prow['pred_sets']\n",
    "\n",
    "    data_all_np5d[iset,ipred, :,:,:, :] = data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_np5d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup dataloader and Dataset for training NN2. Based in Kaggle solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_np5d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.transpose( data_all_np5d , axes=(0,3,4,5,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0.shape[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat_for_mlp = p0.reshape( (np.prod(p0.shape[:4]), p0.shape[4]*p0.shape[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat_for_mlp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. Note that the MLP input must have 12*3= 36 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabels_list_np = np.array(trainlabels_list)\n",
    "trainlabels_list_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_flat_for_mlp = trainlabels_list_np.ravel()\n",
    "label_flat_for_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= torch.from_numpy(data_flat_for_mlp).float()\n",
    "y_train= torch.from_numpy(label_flat_for_mlp).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices = torch.randperm(X_train.shape[0])[:nn2_ntrain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that TensorDataset will create X,y samples by indexing in the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subset = X_train[subset_indices,:].to(cuda_str)\n",
    "y_train_subset = y_train[subset_indices].to(cuda_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dataset = TensorDataset(X_train_subset, y_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subset_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2_train_loader = DataLoader(subset_dataset, batch_size=nn2_batch_size, shuffle=True)\n",
    "# no need for random as it has already been randomized ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2_train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MLP based in the number of input channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    # MLP classifier with sigmoid activation\n",
    "\n",
    "    # Should I add softmax?\n",
    "    def __init__(self, input_size:int, hiden_sizes_list:list, output_size:int, activ_str:str):\n",
    "        super().__init__()\n",
    "\n",
    "        size0= input_size\n",
    "\n",
    "        self.hidden = nn.ModuleList()\n",
    "\n",
    "        for hls in hiden_sizes_list:\n",
    "            hid_layer0 =  nn.Linear(size0, hls)\n",
    "            self.hidden.append(hid_layer0)\n",
    "            size0=hls\n",
    "        #last layer\n",
    "        self.hidden.append(nn.Linear(size0, output_size))\n",
    "\n",
    "        if \"tanh\" in activ_str.lower():\n",
    "            self.activ = nn.functional.tanh\n",
    "        elif \"relu\" in activ_str.lower():\n",
    "            self.activ = nn.functional.relu\n",
    "        elif \"sigm\" in activ_str.lower():\n",
    "            self.activ = nn.functional.sigmoid\n",
    "        else:\n",
    "            raise ValueError(f\"activ_str {activ_str} not valid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # for i,hlayer in self.hidden:\n",
    "        #     x= self.activ(hlayer(x))\n",
    "        for i in range(len(self.hidden)-1):\n",
    "            x= self.activ(self.hidden[i](x))\n",
    "        \n",
    "        #Last layer\n",
    "        x = self.hidden[-1](x)\n",
    "        \n",
    "        #x = self.sigm(x)\n",
    "        return x #returns logits\n",
    "    \n",
    "    # def predict_class_as_cpu_np(self,x):\n",
    "    #     p0 = self.forward(x)\n",
    "    #     pred = torch.squeeze(torch.argmax(p0, dim=1))\n",
    "    #     return pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn2_ptmodel_from_class_generator(nn2_cls_gen_dict: dict ):\n",
    "    hid_layers = nn2_cls_gen_dict['nn2_hidden_layer_sizes'].split(\",\")\n",
    "\n",
    "    if len(hid_layers)==0:\n",
    "        ValueError(f\"Invalid nn2_hidden_layer_sizes : {nn2_cls_gen_dict['nn2_hidden_layer_sizes']}\")\n",
    "\n",
    "    hid_layers_num_list = list(map(int, hid_layers))\n",
    "    logging.info(f\"hid_layers_num_list: {hid_layers_num_list}\")\n",
    "    \n",
    "    model0 = MLPClassifier(\n",
    "        nn2_cls_gen_dict['nn2_in_nchannels'],\n",
    "        hid_layers_num_list,\n",
    "        nn2_cls_gen_dict['nn2_out_nclasses'],\n",
    "        nn2_cls_gen_dict[\"nn2_activation\"]\n",
    "        )\n",
    "        \n",
    "    return model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn2_MLP_models_class_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m NN2_model_fusion \u001b[38;5;241m=\u001b[39m create_nn2_ptmodel_from_class_generator(\u001b[43mnn2_MLP_models_class_generator\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn2_MLP_models_class_generator' is not defined"
     ]
    }
   ],
   "source": [
    "NN2_model_fusion = create_nn2_ptmodel_from_class_generator(nn2_MLP_models_class_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2_model_fusion.to(cuda_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp_test = torch.from_numpy(np.random.random(size=(4096, 36)).astype(np.float32)) # batches, 12 inputs each\n",
    "inp_test = torch.from_numpy(np.random.random(size=(4096, 36))).float().to(cuda_str) # batches, 12 inputs each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2_model_fusion.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2_model_fusion(inp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup optimizer and scaler\n",
    "model=NN2_model_fusion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=nn2_lr)\n",
    "scaler=torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = nn2_train_epochs\n",
    "#epochs = 10\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr= nn2_max_lr,\n",
    "    steps_per_epoch=len(nn2_train_loader),\n",
    "    epochs=epochs,\n",
    "    #pct_start=0.1, #default=0.3\n",
    "    )\n",
    "\n",
    "nn2_loss_func_and_activ= {\"func\": nn.CrossEntropyLoss(), \"activ\":None}\n",
    "# activ = torch.nn.Sigmoid() we may need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model,\n",
    "    nn2_train_loader,\n",
    "    None, # use train data as test?\n",
    "    nn2_loss_func_and_activ,\n",
    "    optimizer, scaler, scheduler,\n",
    "    epochs=epochs,\n",
    "    metric_fn=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the 3 models and the MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "DATE=str(datetime.date.today())\n",
    "DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME=f\"{datetime.datetime.now().hour:02d}{datetime.datetime.now().minute:02d}\"\n",
    "TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_stem=f\"{DATE}_{TIME}\"\n",
    "fname_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = f\"{fname_stem}_model.lgsegm2\"\n",
    "model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN1_models_state_dict = []\n",
    "# for i,m in enumerate(NN1_models):\n",
    "#     #NN1_models_dict[str(i)] = m.state_dict()\n",
    "#     NN1_models_state_dict.append( m.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN1_models_state_dict = [ m.state_dict() for m in NN1_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN1_models_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = f\"\"\"\n",
    "nn1_loss_criterion: {nn1_loss_criterion}\n",
    "nn1_eval_metric: {nn1_eval_metric}\n",
    "nn1_lr: {nn1_lr}\n",
    "nn1_max_lr: {nn1_max_lr}\n",
    "nn1_epochs: {nn1_epochs}\n",
    "\n",
    "nn1_batch_size = 2\n",
    "nn1_num_workers = 1\n",
    "\n",
    "nn2_ntrain: {nn2_ntrain}\n",
    "nn2_train_epochs: {nn2_train_epochs}\n",
    "nn2_batch_size: {nn2_batch_size}\n",
    "nn2_lr: {nn2_lr}\n",
    "nn2_max_lr: {nn2_max_lr}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_save={\n",
    "    \"nn1_models_class_generator\": nn1_models_class_generator,\n",
    "    \"nn1_axes_to_models_indices\": nn1_axes_to_models_indices,\n",
    "    \"data_vol_norm_process\": data_vol_norm_process,\n",
    "    \"NN1_models_state_dict\": NN1_models_state_dict,\n",
    "\n",
    "    \"nn2_MLP_models_class_generator\": nn2_MLP_models_class_generator,\n",
    "    \"NN2_model_dict\":NN2_model_fusion.state_dict(),\n",
    "\n",
    "    \"train_info\": train_info\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dict_to_save, model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN2 predictions with one volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_np5d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_all_np5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4d = data_all_np5d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=data_4d.shape\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0= data_all_np5d[0].reshape( (s[0]*s[1], np.prod(s[2:])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat_for_mlp= p0.transpose((1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topred_tc= torch.from_numpy(data_flat_for_mlp).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tc_ds = TensorDataset(topred_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tc_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tc_batcher = DataLoader(data_tc_ds, batch_size=4096, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,data_batch0 in enumerate(data_tc_batcher):\n",
    "    #res= torch.squeeze(mlp_model(data_multi_preds_probs_np))\n",
    "    print(i, data_batch0[0].shape)\n",
    "    \n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_tc_batcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2_model_fusion.to(\"cpu\")\n",
    "NN2_model_fusion.eval()\n",
    "res_s=[]\n",
    "with torch.no_grad():\n",
    "    logging.info(\"Beggining NN2 inference of whole volume\")\n",
    "    for data_batch in tqdm(data_tc_batcher):\n",
    "        #res= torch.squeeze(mlp_model(data_multi_preds_probs_np))\n",
    "        pred = NN2_model_fusion(data_batch[0])\n",
    "        pred_argmax = torch.argmax(pred,dim=1)\n",
    "        res_s.append(pred_argmax)\n",
    "        #gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = torch.concatenate(res_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r0.detach().cpu().numpy().reshape(*s[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels0 = trainlabels_list[0]\n",
    "labels0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages=4\n",
    "fig,axs = plt.subplots(2,nimages,figsize=(12,6))\n",
    "\n",
    "randomints= np.random.default_rng().permutation(256)\n",
    "#plt.tight_layout()\n",
    "for i in range(nimages):\n",
    "    r0 = randomints[i]\n",
    "    datai = data_to_predict[r0,:,:]\n",
    "    pred_labeli = r2[r0,:,:]\n",
    "    gnd_labeli = labels0[r0,:,:]\n",
    "    axs[0,i].imshow(datai, cmap=\"gray\")\n",
    "    axs[0,i].set_axis_off()\n",
    "    axs[0,i].imshow(pred_labeli , cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[0,i].set_axis_off()\n",
    "    axs[1,i].imshow(datai, cmap=\"gray\")\n",
    "    axs[1,i].set_axis_off()\n",
    "    axs[1,i].imshow(gnd_labeli , cmap='tab10', alpha=0.5, vmax=10)\n",
    "    axs[1,i].set_axis_off()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite good results when looking along Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "NV = napari.Viewer()\n",
    "NV.add_image(data_to_predict)\n",
    "NV.add_labels(r2)\n",
    "NV.add_labels(labels0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and run predictions\n",
    "\n",
    "From restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "#import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "import tempfile\n",
    "import logging\n",
    "from types import SimpleNamespace\n",
    "import tqdm #progress bar in iterations\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import albumentations as alb\n",
    "import albumentations.pytorch\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = torch.load(\"2024-06-15_2054_model.lgsegm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_str = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn1_ptmodel_from_class_generator(nn1_cls_gen_dict: dict):\n",
    "    # get segm model from dictionary item\n",
    "    model0=None\n",
    "\n",
    "    if nn1_cls_gen_dict['class'].lower()=='smp': #unet, AttentionNet (manet) and fpn\n",
    "        #Segmentation models pytorch\n",
    "        arch = nn1_cls_gen_dict['arch'].lower()\n",
    "        if arch==\"unet\" or arch==\"u_net\":\n",
    "            NN_class = smp.Unet\n",
    "        elif arch==\"manet\":\n",
    "            model0 = smp.MAnet\n",
    "        elif arch==\"fpn\":\n",
    "            model0 = smp.FPN\n",
    "        else:\n",
    "            raise ValueError(f\"arch:{arch} not valid.\")\n",
    "        \n",
    "        model0 = NN_class(\n",
    "            encoder_name = nn1_cls_gen_dict['encoder_name'],\n",
    "            encoder_weights = nn1_cls_gen_dict['encoder_weights'],\n",
    "            in_channels = nn1_cls_gen_dict['in_nchannels'],\n",
    "            classes = nn1_cls_gen_dict['nclasses'],\n",
    "            #activation = \"sigmoid\" # Whether to use activation or not, depends whether the loss function require slogits or not\n",
    "            activation = None\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"class {nn1_cls_gen_dict['class']} not supported.\")\n",
    "    \n",
    "    # TODO: add other 2D model support, not just SMPs\n",
    "\n",
    "    return model0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create NN1 models from loaded dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model[\"NN1_models_state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(load_model[\"NN1_models_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model[\"nn1_models_class_generator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN1_models=[]\n",
    "# for i,w0 in load_model[\"NN1_models_dict\"].items():\n",
    "#     cg0= load_model[\"nn1_models_class_generator\"][int(i)]\n",
    "#     m0 = create_nn1_ptmodel_from_class_generator(cg0)\n",
    "#     m0.load_state_dict(w0)\n",
    "#     NN1_models.append( m0.to(cuda_str) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN1_models=[]\n",
    "for cg0,w0 in zip(load_model[\"nn1_models_class_generator\"], load_model[\"NN1_models_state_dict\"] ):\n",
    "    cg0['encoder_weights'] = None # Ensure no weights are preloaded\n",
    "    m0 = create_nn1_ptmodel_from_class_generator(cg0)\n",
    "    m0.load_state_dict(w0)\n",
    "    NN1_models.append( m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN1_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [ m.to(cuda_str) for m in NN1_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN2 (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    # MLP classifier with sigmoid activation\n",
    "\n",
    "    # Should I add softmax?\n",
    "    def __init__(self, input_size:int, hiden_sizes_list:list, output_size:int, activ_str:str):\n",
    "        super().__init__()\n",
    "\n",
    "        size0= input_size\n",
    "\n",
    "        self.hidden = nn.ModuleList()\n",
    "\n",
    "        for hls in hiden_sizes_list:\n",
    "            hid_layer0 =  nn.Linear(size0, hls)\n",
    "            self.hidden.append(hid_layer0)\n",
    "            size0=hls\n",
    "        #last layer\n",
    "        self.hidden.append(nn.Linear(size0, output_size))\n",
    "\n",
    "        if \"tanh\" in activ_str.lower():\n",
    "            self.activ = nn.functional.tanh\n",
    "        elif \"relu\" in activ_str.lower():\n",
    "            self.activ = nn.functional.relu\n",
    "        elif \"sigm\" in activ_str.lower():\n",
    "            self.activ = nn.functional.sigmoid\n",
    "        else:\n",
    "            raise ValueError(f\"activ_str {activ_str} not valid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # for i,hlayer in self.hidden:\n",
    "        #     x= self.activ(hlayer(x))\n",
    "        for i in range(len(self.hidden)-1):\n",
    "            x= self.activ(self.hidden[i](x))\n",
    "        \n",
    "        #Last layer\n",
    "        x = self.hidden[-1](x)\n",
    "        \n",
    "        #x = self.sigm(x)\n",
    "        return x #returns logits\n",
    "    \n",
    "    # def predict_class_as_cpu_np(self,x):\n",
    "    #     p0 = self.forward(x)\n",
    "    #     pred = torch.squeeze(torch.argmax(p0, dim=1))\n",
    "    #     return pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn2_ptmodel_from_class_generator(nn2_cls_gen_dict: dict ):\n",
    "    hid_layers = nn2_cls_gen_dict['nn2_hidden_layer_sizes'].split(\",\")\n",
    "\n",
    "    if len(hid_layers)==0:\n",
    "        ValueError(f\"Invalid nn2_hidden_layer_sizes : {nn2_cls_gen_dict['nn2_hidden_layer_sizes']}\")\n",
    "\n",
    "    hid_layers_num_list = list(map(int, hid_layers))\n",
    "    logging.info(f\"hid_layers_num_list: {hid_layers_num_list}\")\n",
    "    \n",
    "    model0 = MLPClassifier(\n",
    "        nn2_cls_gen_dict['nn2_in_nchannels'],\n",
    "        hid_layers_num_list,\n",
    "        nn2_cls_gen_dict['nn2_out_nclasses'],\n",
    "        nn2_cls_gen_dict[\"nn2_activation\"]\n",
    "        )\n",
    "    \n",
    "    if \"NN2_model_dict\" in nn2_cls_gen_dict.keys():\n",
    "        logging.info(\"NN2: load weights from dict\")\n",
    "        model0.load_state_dict(nn2_cls_gen_dict[\"NN2_model_dict\"])\n",
    "        \n",
    "    return model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2_dict = load_model[\"nn2_MLP_models_class_generator\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2_model_fusion = create_nn2_ptmodel_from_class_generator(load_model[\"nn2_MLP_models_class_generator\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2_model_fusion.load_state_dict(load_model[\"NN2_model_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2_model_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_axes_to_models_indices = load_model[\"nn1_axes_to_models_indices\"]\n",
    "nn1_axes_to_models_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_str = cuda_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_batch_size=2 # does not load but needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Run predictions using these models\n",
    "\n",
    "Collect functions from header \"Predict volume(s) using the models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "def _save_pred_data(folder, data, count,axis, rot):\n",
    "    # Saves predicted data to h5 file in tempdir and return file path in case it is needed\n",
    "    file_path = f\"{folder}/pred_{count}_{axis}_{rot}.h5\"\n",
    "\n",
    "    logging.info(f\"Saving data of shape {data.shape} to {file_path}.\")\n",
    "    with h5py.File(file_path, \"w\") as f:\n",
    "        f.create_dataset(\"/data\", data=data)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "class VolumeSlicerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datavol, axis, per_slice_tfms=None, device_str=\"cuda:0\"):\n",
    "        assert datavol.ndim==3\n",
    "        assert axis==0 or axis==1 or axis==2\n",
    "\n",
    "        self.datavol=datavol\n",
    "        self.axis=axis\n",
    "        self.per_slice_tfms=per_slice_tfms\n",
    "        self.device_str = device_str\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datavol.shape[self.axis]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data_slice=None\n",
    "        if self.axis==0:\n",
    "            data_slice = self.datavol[idx,:,:]\n",
    "        elif self.axis==1:\n",
    "            data_slice = self.datavol[:,idx,:]\n",
    "        elif self.axis==2:\n",
    "            data_slice = self.datavol[:,:,idx]\n",
    "\n",
    "        res = data_slice\n",
    "        # Apply transform\n",
    "        if self.per_slice_tfms is not None:\n",
    "            res = self.per_slice_tfms(data_slice)\n",
    "\n",
    "        #Convert to tensor and send to device\n",
    "        res_torch = torch.unsqueeze(torch.from_numpy(res), dim=0).float().to(self.device_str)\n",
    "\n",
    "        return res_torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn1_predict_slices_along_axis_1(datavol, axis):\n",
    "    ds0 = VolumeSlicerDataset(datavol, axis , per_slice_tfms=None, device_str=device_str)\n",
    "    dl0 = DataLoader(dataset=ds0, batch_size=nn1_batch_size, shuffle=False)\n",
    "\n",
    "    # Get correct model\n",
    "    model_index = nn1_axes_to_models_indices[axis]\n",
    "    #model = NN1_models[model_index]\n",
    "    model = NN1_models[model_index]\n",
    "    logging.info(f\"axis:{axis}, use model_index: {model_index}\")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    SM_func = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    for ibatch, x in enumerate(dl0):\n",
    "        # x.shape is (batchsize, 1, 256,256) with 256 being the imagesize\n",
    "        X= model(x)\n",
    "        #pred shape is (batchsize, 3, 256, 256)\n",
    "\n",
    "        pred_probs_slice = SM_func(X) #Convert to probabilities\n",
    "\n",
    "        # get labels using argmax\n",
    "        lbl_slice = torch.argmax(pred_probs_slice, dim=1)\n",
    "        #labels_list.append(lbl_slice)\n",
    "\n",
    "        # need to move away from device, otherwise it uses too much VRAM\n",
    "        pred_probs_slice_np = pred_probs_slice.detach().cpu().numpy()\n",
    "        lbl_slice_np = lbl_slice.detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        preds_list.append(pred_probs_slice_np)\n",
    "        labels_list.append(lbl_slice_np)\n",
    "\n",
    "    logging.info(\"Prediction of all slices complete. Now stacking and getting the right orientation.\")\n",
    "    # stack slices\n",
    "    preds_list_conc = np.concatenate(preds_list, axis=0) # shape will be (256,3,256,256)\n",
    "    labels_pred_conc = np.concatenate(labels_list, axis=0)\n",
    "\n",
    "    pred_oriented = None\n",
    "    labels_oriented = None\n",
    "    if axis==0:\n",
    "        pred_oriented = np.transpose(preds_list_conc, axes=(1,0,2,3))\n",
    "        labels_oriented = labels_pred_conc # no need to orient\n",
    "    elif axis==1:\n",
    "        pred_oriented = np.transpose(preds_list_conc, axes=(1,2,0,3))\n",
    "        labels_oriented = np.transpose(labels_pred_conc, axes=(1,0,2))\n",
    "    elif axis==2:\n",
    "        pred_oriented = np.transpose(preds_list_conc, axes=(1,2,3,0))\n",
    "        labels_oriented = np.transpose(labels_pred_conc, axes=(1,2,0))\n",
    "\n",
    "    #with pred_oriented note that class probability is at the start\n",
    "    return pred_oriented, labels_oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_NN1(data_to_predict_l, path_out_results):\n",
    "\n",
    "\n",
    "    pred_data_probs_filenames=[] #Will store results in files, and keep the filenames as reference\n",
    "    pred_data_labels_filenames=[]\n",
    "    pred_sets=[]\n",
    "    pred_planes=[]\n",
    "    pred_rots=[]\n",
    "    pred_ipred=[]\n",
    "    pred_shapes=[]\n",
    "    itag=0\n",
    "\n",
    "    for iset, data_to_predict in enumerate(data_to_predict_l):\n",
    "        logging.info(f\"Data to predict iset:{iset}\")\n",
    "        #data_vol = np.array(data_to_predict0) #Copies\n",
    "\n",
    "        for krot in range(0, 4): #Around axis rotations\n",
    "            rot_angle_degrees = krot * 90\n",
    "            logging.info(f\"Volume to be rotated by {rot_angle_degrees} degrees\")\n",
    "\n",
    "            #Predict 3 axis\n",
    "            #YX, along Z\n",
    "            # planeYX=(1,2)\n",
    "            logging.info(\"Predicting YX slices, along Z\")\n",
    "            data_vol = np.array(np.rot90(data_to_predict,krot, axes=(1,2))) #rotate\n",
    "\n",
    "            #prob0,lab0 = nn1_predict_slices_along_axis(data_vol, axis=0, device_str=cuda_str)\n",
    "            prob0,lab0 = nn1_predict_slices_along_axis_1(data_vol, 0)\n",
    "\n",
    "            #invert rotations before saving\n",
    "            pred_probs = np.rot90(prob0, -krot, axes=(2,3)) \n",
    "            pred_labels = np.rot90(lab0, -krot, axes=(1,2)) #note that class is at start\n",
    "\n",
    "            fn = _save_pred_data(path_out_results,pred_probs, iset, \"YX\", rot_angle_degrees)\n",
    "            pred_data_probs_filenames.append(fn)\n",
    "            fn = _save_pred_data(path_out_results,pred_labels, iset, \"YX_labels\", rot_angle_degrees)\n",
    "            pred_data_labels_filenames.append(fn)\n",
    "            \n",
    "            pred_sets.append(iset)\n",
    "            pred_planes.append(\"YX\")\n",
    "            pred_rots.append(rot_angle_degrees)\n",
    "            pred_ipred.append(itag)\n",
    "            pred_shapes.append(pred_labels.shape)\n",
    "            itag+=1\n",
    "\n",
    "\n",
    "\n",
    "            #ZX\n",
    "            logging.info(\"Predicting ZX slices, along Y\")\n",
    "            #planeZX=(0,2)\n",
    "            data_vol = np.array(np.rot90(data_to_predict,krot, axes=(0,2))) #rotate\n",
    "            #prob0,lab0 = nn1_predict_slices_along_axis(data_vol, axis=1, device_str=cuda_str)\n",
    "            prob0,lab0 = nn1_predict_slices_along_axis_1(data_vol, 1)\n",
    "\n",
    "\n",
    "            pred_probs = np.rot90(prob0, -krot, axes=(1,3)) #invert rotation before saving\n",
    "            pred_labels = np.rot90(lab0, -krot, axes=(0,2))\n",
    "\n",
    "            fn = _save_pred_data(path_out_results,pred_probs, iset, \"ZX\", rot_angle_degrees)\n",
    "            pred_data_probs_filenames.append(fn)\n",
    "            fn = _save_pred_data(path_out_results,pred_labels, iset, \"ZX_labels\", rot_angle_degrees)\n",
    "            pred_data_labels_filenames.append(fn)\n",
    "            \n",
    "            pred_sets.append(iset)\n",
    "            pred_planes.append(\"ZX\")\n",
    "            pred_rots.append(rot_angle_degrees)\n",
    "            pred_ipred.append(itag)\n",
    "            pred_shapes.append(pred_labels.shape)\n",
    "            itag+=1\n",
    "\n",
    "\n",
    "\n",
    "            #ZY\n",
    "            logging.info(\"Predicting ZY slices, along X\")\n",
    "            #planeZY=(0,1)\n",
    "            data_vol = np.array(np.rot90(data_to_predict,krot, axes=(0,1))) #rotate\n",
    "            #prob0,lab0 = nn1_predict_slices_along_axis(data_vol, axis=2, device_str=cuda_str)\n",
    "            prob0,lab0 = nn1_predict_slices_along_axis_1(data_vol, 2)\n",
    "\n",
    "            pred_probs = np.rot90(prob0, -krot, axes=(1,2)) #invert rotation before saving\n",
    "            pred_labels = np.rot90(lab0, -krot, axes=(0,1))\n",
    "            \n",
    "            fn = _save_pred_data(path_out_results,pred_probs, iset, \"ZY\", rot_angle_degrees)\n",
    "            pred_data_probs_filenames.append(fn)\n",
    "            fn = _save_pred_data(path_out_results,pred_labels, iset, \"ZY_labels\", rot_angle_degrees)\n",
    "            pred_data_labels_filenames.append(fn)\n",
    "            \n",
    "            pred_sets.append(iset)\n",
    "            pred_planes.append(\"ZY\")\n",
    "            pred_rots.append(rot_angle_degrees)\n",
    "            pred_ipred.append(itag)\n",
    "            pred_shapes.append(pred_labels.shape)\n",
    "            itag+=1\n",
    "\n",
    "    all_pred_pd = pd.DataFrame({\n",
    "        'pred_data_probs_filenames': pred_data_probs_filenames,\n",
    "        'pred_data_labels_filenames': pred_data_labels_filenames,\n",
    "        'pred_sets':pred_sets,\n",
    "        'pred_planes':pred_planes,\n",
    "        'pred_rots':pred_rots,\n",
    "        'pred_ipred':pred_ipred,\n",
    "        'pred_shapes': pred_shapes,\n",
    "    })\n",
    "\n",
    "    return all_pred_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load validation data which will be used to test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_l = []\n",
    "\n",
    "val_data = tifffile.imread(\"test_data\\TS_0005_crop_val.tif\")\n",
    "val_labels_gnd = tifffile.imread(\"test_data\\TS_0005_ribos_membr_crop_val.tif\")\n",
    "\n",
    "val_data_l = [val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_data(d0):\n",
    "    d0_mean = np.mean(d0)\n",
    "    d0_std = np.std(d0)\n",
    "\n",
    "    if d0_std==0:\n",
    "        raise ValueError(\"Error. Stdev of data volume is zero.\")\n",
    "    \n",
    "    d0_corr = (d0.astype(np.float32) - d0_mean) / d0_std\n",
    "    d0_corr = (np.clip(d0_corr, -3.0, 3.0) +3.0) / 6.0\n",
    "    \n",
    "    return (d0_corr*255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_l_corr = [ correct_data(d) for d in val_data_l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tempdir here. If created inside the function it will be deleted after returning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir_pred= tempfile.TemporaryDirectory()\n",
    "path_out_results = Path(tempdir_pred.name)\n",
    "logging.info(f\"tempdir_pred_path:{path_out_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pd = predict_NN1(val_data_l_corr, path_out_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pd[\"pred_ipred\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict NN2 from pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_from_pd(all_pred_pd):\n",
    "    data_all_np5d=None\n",
    "\n",
    "    logging.debug(\"Aggregating multiple sets onto a single volume data_all_np5d\")\n",
    "    # aggregate multiple sets for data\n",
    "    for i,prow in all_pred_pd.iterrows():\n",
    "\n",
    "        prob_filename = prow['pred_data_probs_filenames']\n",
    "        with h5py.File(prob_filename,'r') as f:\n",
    "            data0 = np.array(f[\"data\"])\n",
    "\n",
    "        if i==0:\n",
    "            #initialise\n",
    "            logging.info(f\"filename:{prob_filename} , shape:{data0.shape}\")\n",
    "            all_shape0 = (\n",
    "                all_pred_pd[\"pred_sets\"].max()+1, # needs to be adjusted\n",
    "                all_pred_pd[\"pred_ipred\"].max()+1, # needs to be adjusted, perhaps can be collected from dataframe\n",
    "                *data0.shape\n",
    "                )\n",
    "\n",
    "            data_all_np5d=np.zeros( all_shape0 , dtype=data0.dtype)\n",
    "\n",
    "        \n",
    "        ipred=prow['pred_ipred']\n",
    "        iset=prow['pred_sets']\n",
    "\n",
    "        data_all_np5d[iset,ipred, :,:,:, :] = data0\n",
    "    \n",
    "    return data_all_np5d\n",
    "\n",
    "def NN2_predict_from_pd(all_pred_pd, device_str=\"cpu\"):\n",
    "\n",
    "    #Collect all data and put it in a very large array\n",
    "    data_all_np5d = aggregate_data_from_pd(all_pred_pd)\n",
    "    logging.info(f\"data_all_np5d.shape: {data_all_np5d.shape}\")\n",
    "\n",
    "    nsets = data_all_np5d.shape[0]\n",
    "    logging.info(f\"nsets: {nsets}\")\n",
    "\n",
    "    nn2_preds = []\n",
    "    for iset in range(nsets):\n",
    "        data_4d = data_all_np5d[iset]\n",
    "        s = data_4d.shape\n",
    "        p0= data_4d.reshape( (s[0]*s[1], np.prod(s[2:])) )\n",
    "        data_flat_for_mlp= p0.transpose((1,0))\n",
    "        topred_tc= torch.from_numpy(data_flat_for_mlp).float().to(device_str)\n",
    "        data_tc_ds = TensorDataset(topred_tc)\n",
    "        data_tc_batcher = DataLoader(data_tc_ds, batch_size=4096, shuffle=False)\n",
    "\n",
    "        NN2_model_fusion.to(device_str)\n",
    "        NN2_model_fusion.eval()\n",
    "        res_s=[]\n",
    "        with torch.no_grad():\n",
    "            logging.info(\"Beggining NN2 inference of whole volume\")\n",
    "            for data_batch in tqdm(data_tc_batcher):\n",
    "                #res= torch.squeeze(mlp_model(data_multi_preds_probs_np))\n",
    "                pred = NN2_model_fusion(data_batch[0])\n",
    "                pred_argmax = torch.argmax(pred,dim=1)\n",
    "                res_s.append(pred_argmax)\n",
    "        r0 = torch.concatenate(res_s)\n",
    "        r2 = r0.detach().cpu().numpy().reshape(*s[2:])\n",
    "        logging.info(f\"iset:{iset}, nn2 prediction shape:{r2.shape}\")\n",
    "\n",
    "        nn2_preds.append(r2)\n",
    "\n",
    "    return nn2_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run NN2 prediction on the result panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2_preds = NN2_predict_from_pd(res_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "NV=napari.Viewer()\n",
    "NV.add_image(val_data)\n",
    "NV.add_labels(val_labels_gnd)\n",
    "NV.add_labels(nn2_preds[0].astype(np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test segmentor2 train_nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import leopardgecko.segmentor2 as lgs2\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s — %(name)s — %(levelname)s — %(funcName)s:%(lineno)d — %(message)s\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 01:32:12,237 — root — INFO — update_nn2_model_from_generator:979 — update_NN2_model_from_generator()\n",
      "2024-07-01 01:32:12,237 — root — INFO — create_nn2_ptmodel_from_class_generator:946 — create_nn2_ptmodel_from_class_generator()\n",
      "2024-07-01 01:32:12,247 — root — INFO — create_nn2_ptmodel_from_class_generator:954 — hid_layers_num_list: [10, 10]\n"
     ]
    }
   ],
   "source": [
    "lgs2.nn1_train_epochs=2 # debug low number\n",
    "lgs2.nn2_train_epochs=2\n",
    "\n",
    "\n",
    "lgs2.nn2_MLP_model_class_generator= lgs2.nn2_MLP_model_class_generator_default\n",
    "\n",
    "lgs2.nn2_ntrain = 8\n",
    "\n",
    "lgs2.update_nn2_model_from_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pretend input for nn2 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data = np.random.random( (2, 12, 3, 64,64,64)) # 2 sets, 12 ways, 3 classes\n",
    "\n",
    "inp_labels = np.random.randint(0,3,size=(2,64,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 01:32:12,554 — root — INFO — train_nn2:1074 — NN2_train()\n",
      "2024-07-01 01:32:12,555 — root — INFO — train_nn2:1075 — data_all_np5d.shape:(2, 12, 3, 64, 64, 64), len(trainlabels_list): 2\n",
      "2024-07-01 01:32:12,556 — root — INFO — train_nn2:1099 — Selecting only nn2_ntrain voxel coordinates from data and ground truth for training\n",
      "2024-07-01 01:32:12,559 — root — INFO — train_nn2:1159 — X_train_subset_t and y_train_subset_t created\n",
      "2024-07-01 01:32:12,559 — root — INFO — train_nn2:1164 — dataset_X_y_train created\n",
      "2024-07-01 01:32:12,559 — root — INFO — train_nn2:1168 — Creating test dataset\n",
      "2024-07-01 01:32:12,565 — root — INFO — train_nn2:1173 — X_test_subset_t and y_test_subset_t created\n",
      "2024-07-01 01:32:12,565 — root — INFO — train_nn2:1178 — dataset_X_y_test created\n",
      "2024-07-01 01:32:12,568 — root — INFO — train_nn2:1233 — Beggining training NN2.\n",
      "2024-07-01 01:32:12,570 — root — INFO — train_model:531 — train_model()\n",
      "2024-07-01 01:32:12,570 — root — INFO — train_model:534 — ---- Epoch 1/2 ----\n",
      "2024-07-01 01:32:16,989 — root — INFO — train_loop:468 — batch:0  loss: 1.113296  [    8/    8]. lr:[2e-07]\n",
      "2024-07-01 01:32:17,026 — root — INFO — test_loop:517 — Avg loss: 1.423602\n",
      "2024-07-01 01:32:17,026 — root — INFO — test_loop:522 — Avg metric: 0.000000\n",
      "2024-07-01 01:32:17,026 — root — INFO — train_model:534 — ---- Epoch 2/2 ----\n",
      "2024-07-01 01:32:17,035 — root — INFO — train_loop:468 — batch:0  loss: 0.993088  [    8/    8]. lr:[0.04058728269748815]\n",
      "2024-07-01 01:32:17,041 — root — INFO — test_loop:517 — Avg loss: 1.423605\n",
      "2024-07-01 01:32:17,052 — root — INFO — test_loop:522 — Avg metric: 0.000000\n",
      "2024-07-01 01:32:17,073 — root — INFO — train_model:541 — Done!\n",
      "2024-07-01 01:32:17,073 — root — INFO — train_model:543 — Final test loss is : 1.42360520362854, and metric is: 0.0\n",
      "2024-07-01 01:32:17,073 — root — INFO — train_nn2:1245 — Training NN2 complete.\n"
     ]
    }
   ],
   "source": [
    "lgs2.train_nn2(inp_data, inp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
