{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import leopardgecko as lg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ceph/users/ypu66991/Programming/Git_LeopardGecko/scripts\n"
     ]
    }
   ],
   "source": [
    "import leopardgecko.segmentor as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "traindata_crop_filename = \"/ceph/users/ypu66991/Analysis/GasHydrate/ROI_1165_1421_512_768_512_768_ManualLabelFix/data_ROI_1165_1421_512_768_512_768.tiff\"\n",
    "trainlabels_crop_filename = \"/ceph/users/ypu66991/Analysis/GasHydrate/ROI_1165_1421_512_768_512_768_ManualLabelFix/labels_manual.tif\"\n",
    "\n",
    "#Loads data\n",
    "print(\"Loading data\")\n",
    "import tifffile\n",
    "traindata = tifffile.imread(traindata_crop_filename)\n",
    "trainlabels = tifffile.imread(trainlabels_crop_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run segmentor with this data\n",
    "\n",
    "#Create the class\n",
    "lgsegmentor0 = sg.cMultiAxisRotationsSegmentor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ceph/users/ypu66991/Programming/Git_LeopardGecko/scripts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'leopardgecko.segmentor' from '/ceph/users/ypu66991/Programming/Git_LeopardGecko/leopardgecko/segmentor.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Calculating mean of data...\n",
      "INFO:root:Mean value: 143.96438401937485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run segmentor train()\n",
      "tempdir_data_path:/tmp/tmp62q3yxl9\n",
      "tempdir_seg_path:/tmp/tmpbr3rvz4c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Number of classes in segmentation dataset: 3\n",
      "INFO:root:These classes are: [0 1 2]\n",
      "INFO:root:Slicing data volume and saving slices to disk\n",
      "100%|██████████| 768/768 [00:04<00:00, 168.82it/s]\n",
      "INFO:root:Slicing label volume and saving slices to disk\n",
      "  0%|          | 0/768 [00:00<?, ?it/s]/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/skimage/util/dtype.py:550: UserWarning: Downcasting uint32 to uint8 without scaling because max value 2 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      " 19%|█▉        | 148/768 [00:01<00:05, 106.20it/s]/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/skimage/util/dtype.py:550: UserWarning: Downcasting uint32 to uint8 without scaling because max value 1 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      "100%|██████████| 768/768 [00:07<00:00, 107.98it/s]\n",
      "INFO:root:Free GPU memory is 31.75 GB. Batch size will be 12.\n",
      "INFO:root:Using DiceLoss\n",
      "INFO:root:Using MeanIoU\n",
      "INFO:root:Setting up the model on device 0.\n",
      "INFO:root:Sending the U-Net model to device 0\n",
      "INFO:root:Freezing model with 24430387 trainable parameters, 24430387 total parameters.\n",
      "INFO:root:Model has 3341043 trainable parameters, 24430387 total parameters.\n",
      "INFO:root:Trainer created.\n",
      "INFO:root:Finding learning rate for model.\n",
      "INFO:root:Training for 1 epochs to create a learning rate plot.\n",
      "Epoch 1, batch number: 100%|██████████████████████████████| 51/51 [00:05<00:00,  8.69it/s]\n",
      "INFO:root:LR to use 0.003970123728284112\n",
      "INFO:root:Setting up the model on device 0.\n",
      "INFO:root:Sending the U-Net model to device 0\n",
      "INFO:root:Freezing model with 24430387 trainable parameters, 24430387 total parameters.\n",
      "INFO:root:Model has 3341043 trainable parameters, 24430387 total parameters.\n",
      "INFO:root:Trainer created.\n",
      "INFO:root:Epoch 1 of 8\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 12.27it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:00<00:00, 13.36it/s]\n",
      "INFO:root:Epoch 1. Training loss: 0.3333105970831478, Validation Loss: 0.2487832628763639. MeanIoU: 0.6662527918815613\n",
      "INFO:root:Time taken for epoch 1: 5.14 seconds\n",
      "INFO:root:Validation loss decreased (inf --> 0.248783).  Saving model ...\n",
      "INFO:root:Epoch 2 of 8\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 12.24it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:01<00:00, 12.47it/s]\n",
      "INFO:root:Epoch 2. Training loss: 0.09735934056487738, Validation Loss: 0.15143919908083403. MeanIoU: 0.6595901846885681\n",
      "INFO:root:Time taken for epoch 2: 5.22 seconds\n",
      "INFO:root:Validation loss decreased (0.248783 --> 0.151439).  Saving model ...\n",
      "INFO:root:Epoch 3 of 8\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 12.46it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:01<00:00, 12.89it/s]\n",
      "INFO:root:Epoch 3. Training loss: 0.08535595384298586, Validation Loss: 0.04325110637224638. MeanIoU: 0.8145315647125244\n",
      "INFO:root:Time taken for epoch 3: 5.11 seconds\n",
      "INFO:root:Validation loss decreased (0.151439 --> 0.043251).  Saving model ...\n",
      "INFO:root:Epoch 4 of 8\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 12.65it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:00<00:00, 13.10it/s]\n",
      "INFO:root:Epoch 4. Training loss: 0.07317080684736663, Validation Loss: 0.11017355552086464. MeanIoU: 0.7560192346572876\n",
      "INFO:root:Time taken for epoch 4: 5.03 seconds\n",
      "INFO:root:EarlyStopping counter: 1 out of 3\n",
      "INFO:root:Epoch 5 of 8\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 12.64it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:01<00:00, 12.10it/s]\n",
      "INFO:root:Epoch 5. Training loss: 0.050638836972853714, Validation Loss: 0.030694768978999212. MeanIoU: 0.8354443907737732\n",
      "INFO:root:Time taken for epoch 5: 5.12 seconds\n",
      "INFO:root:Validation loss decreased (0.043251 --> 0.030695).  Saving model ...\n",
      "INFO:root:Epoch 6 of 8\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 12.61it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:01<00:00, 12.10it/s]\n",
      "INFO:root:Epoch 6. Training loss: 0.055356439422158635, Validation Loss: 0.02622279295554528. MeanIoU: 0.8483585119247437\n",
      "INFO:root:Time taken for epoch 6: 5.13 seconds\n",
      "INFO:root:Validation loss decreased (0.030695 --> 0.026223).  Saving model ...\n",
      "INFO:root:Epoch 7 of 8\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 12.59it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:00<00:00, 13.38it/s]\n",
      "INFO:root:Epoch 7. Training loss: 0.044620693898668476, Validation Loss: 0.02280119749215933. MeanIoU: 0.8522332310676575\n",
      "INFO:root:Time taken for epoch 7: 5.03 seconds\n",
      "INFO:root:Validation loss decreased (0.026223 --> 0.022801).  Saving model ...\n",
      "INFO:root:Epoch 8 of 8\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:03<00:00, 12.75it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:01<00:00, 12.01it/s]\n",
      "INFO:root:Epoch 8. Training loss: 0.03996959971446617, Validation Loss: 0.021982032519120436. MeanIoU: 0.8530422449111938\n",
      "INFO:root:Time taken for epoch 8: 5.09 seconds\n",
      "INFO:root:Validation loss decreased (0.022801 --> 0.021982).  Saving model ...\n",
      "INFO:root:Loading model weights.\n",
      "INFO:root:Setting up the model on device 0.\n",
      "INFO:root:Sending the U-Net model to device 0\n",
      "INFO:root:Model has 24430387 trainable parameters, 24430387 total parameters.\n",
      "INFO:root:Trainer created.\n",
      "INFO:root:Loading in weights from saved checkpoint.\n",
      "INFO:root:Loading model weights.\n",
      "INFO:root:Finding learning rate for model.\n",
      "INFO:root:Training for 1 epochs to create a learning rate plot.\n",
      "Epoch 1, batch number: 100%|██████████████████████████████| 51/51 [00:04<00:00, 10.41it/s]\n",
      "INFO:root:LR to use 1.8914258901461455e-09\n",
      "INFO:root:Setting up the model on device 0.\n",
      "INFO:root:Sending the U-Net model to device 0\n",
      "INFO:root:Model has 24430387 trainable parameters, 24430387 total parameters.\n",
      "INFO:root:Trainer created.\n",
      "INFO:root:Loading in weights from saved checkpoint.\n",
      "INFO:root:Loading model weights.\n",
      "INFO:root:Epoch 1 of 5\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 11.66it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:00<00:00, 14.64it/s]\n",
      "INFO:root:Epoch 1. Training loss: 0.04009407291225359, Validation Loss: 0.02226948279600877. MeanIoU: 0.8521751165390015\n",
      "INFO:root:Time taken for epoch 1: 5.27 seconds\n",
      "INFO:root:EarlyStopping counter: 1 out of 3\n",
      "INFO:root:Epoch 2 of 5\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 11.67it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:00<00:00, 14.35it/s]\n",
      "INFO:root:Epoch 2. Training loss: 0.03827826649534936, Validation Loss: 0.022061104957874004. MeanIoU: 0.852702260017395\n",
      "INFO:root:Time taken for epoch 2: 5.29 seconds\n",
      "INFO:root:EarlyStopping counter: 2 out of 3\n",
      "INFO:root:Epoch 3 of 5\n",
      "Training batch: 100%|██████████████████████████████| 51/51 [00:04<00:00, 11.48it/s]\n",
      "Validation batch: 100%|██████████████████████████████| 13/13 [00:00<00:00, 14.90it/s]\n",
      "INFO:root:Epoch 3. Training loss: 0.037883376373964196, Validation Loss: 0.02207071964557354. MeanIoU: 0.8534047603607178\n",
      "INFO:root:Time taken for epoch 3: 5.33 seconds\n",
      "INFO:root:EarlyStopping counter: 3 out of 3\n",
      "INFO:root:Early stopping\n",
      "INFO:root:Loading model weights.\n",
      "INFO:root:Deleting 768 images.\n",
      "INFO:root:Deleting the empty directory.\n",
      "INFO:root:Deleting 768 images.\n",
      "INFO:root:Deleting the empty directory.\n",
      "INFO:root:Loading model dictionary from file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempdir_pred_path:/tmp/tmpasvy0u2w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sending the U-Net model to device 0\n",
      "INFO:root:Loading in the saved weights.\n",
      "INFO:root:Volume rotated by 0 degrees\n",
      "INFO:root:Predicting YX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 61.10it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_YX_0.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_YX_labels_0.h5.\n",
      "INFO:root:Predicting ZX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 59.75it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_ZX_0.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_ZX_labels_0.h5.\n",
      "INFO:root:Predicting ZY slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 36.81it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_ZY_0.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_ZY_labels_0.h5.\n",
      "INFO:root:Volume rotated by 90 degrees\n",
      "INFO:root:Predicting YX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 59.16it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_YX_90.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_YX_labels_90.h5.\n",
      "INFO:root:Predicting ZX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 61.15it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_ZX_90.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_ZX_labels_90.h5.\n",
      "INFO:root:Predicting ZY slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 41.16it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_ZY_90.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_ZY_labels_90.h5.\n",
      "INFO:root:Volume rotated by 180 degrees\n",
      "INFO:root:Predicting YX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 45.50it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_YX_180.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_YX_labels_180.h5.\n",
      "INFO:root:Predicting ZX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 45.54it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_ZX_180.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_ZX_labels_180.h5.\n",
      "INFO:root:Predicting ZY slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 40.74it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_ZY_180.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_ZY_labels_180.h5.\n",
      "INFO:root:Volume rotated by 270 degrees\n",
      "INFO:root:Predicting YX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 45.60it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_YX_270.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_YX_labels_270.h5.\n",
      "INFO:root:Predicting ZX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 45.99it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_ZX_270.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_ZX_labels_270.h5.\n",
      "INFO:root:Predicting ZY slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (256, 256, 256).\n",
      "Prediction batch: 100%|██████████████████████████████| 64/64 [00:01<00:00, 41.02it/s]\n",
      "INFO:root:Saving data of shape (256, 256, 256, 3) to /tmp/tmpasvy0u2w/pred_ZY_270.h5.\n",
      "INFO:root:Saving data of shape (256, 256, 256) to /tmp/tmpasvy0u2w/pred_ZY_labels_270.h5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_data_probs_filenames\n",
      "['/tmp/tmpasvy0u2w/pred_YX_0.h5', '/tmp/tmpasvy0u2w/pred_ZX_0.h5', '/tmp/tmpasvy0u2w/pred_ZY_0.h5', '/tmp/tmpasvy0u2w/pred_YX_90.h5', '/tmp/tmpasvy0u2w/pred_ZX_90.h5', '/tmp/tmpasvy0u2w/pred_ZY_90.h5', '/tmp/tmpasvy0u2w/pred_YX_180.h5', '/tmp/tmpasvy0u2w/pred_ZX_180.h5', '/tmp/tmpasvy0u2w/pred_ZY_180.h5', '/tmp/tmpasvy0u2w/pred_YX_270.h5', '/tmp/tmpasvy0u2w/pred_ZX_270.h5', '/tmp/tmpasvy0u2w/pred_ZY_270.h5']\n",
      "pred_data_labels_filenames\n",
      "['/tmp/tmpasvy0u2w/pred_YX_labels_0.h5', '/tmp/tmpasvy0u2w/pred_ZX_labels_0.h5', '/tmp/tmpasvy0u2w/pred_ZY_labels_0.h5', '/tmp/tmpasvy0u2w/pred_YX_labels_90.h5', '/tmp/tmpasvy0u2w/pred_ZX_labels_90.h5', '/tmp/tmpasvy0u2w/pred_ZY_labels_90.h5', '/tmp/tmpasvy0u2w/pred_YX_labels_180.h5', '/tmp/tmpasvy0u2w/pred_ZX_labels_180.h5', '/tmp/tmpasvy0u2w/pred_ZY_labels_180.h5', '/tmp/tmpasvy0u2w/pred_YX_labels_270.h5', '/tmp/tmpasvy0u2w/pred_ZX_labels_270.h5', '/tmp/tmpasvy0u2w/pred_ZY_labels_270.h5']\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n",
      "INFO:root:dicescores = [0.96615345 0.97214472]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:0 , filename: /tmp/tmpasvy0u2w/pred_YX_labels_0.h5, accuracy:0.9822466373443604, dice:0.9691490848264754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.96065826 0.97467423]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:1 , filename: /tmp/tmpasvy0u2w/pred_ZX_labels_0.h5, accuracy:0.979475200176239, dice:0.9676662443004211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.96291451 0.97395442]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:2 , filename: /tmp/tmpasvy0u2w/pred_ZY_labels_0.h5, accuracy:0.9805059432983398, dice:0.9684344603257344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.96065826 0.97467423]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:3 , filename: /tmp/tmpasvy0u2w/pred_YX_labels_90.h5, accuracy:0.979475200176239, dice:0.9676662443004211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.96729291 0.97591009]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:4 , filename: /tmp/tmpasvy0u2w/pred_ZX_labels_90.h5, accuracy:0.9828446507453918, dice:0.9716014986651174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.96317369 0.97476511]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:5 , filename: /tmp/tmpasvy0u2w/pred_ZY_labels_90.h5, accuracy:0.9807060956954956, dice:0.9689693994378239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.96729291 0.97591009]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:6 , filename: /tmp/tmpasvy0u2w/pred_YX_labels_180.h5, accuracy:0.9828446507453918, dice:0.9716014986651174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.95825765 0.96789893]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:7 , filename: /tmp/tmpasvy0u2w/pred_ZX_labels_180.h5, accuracy:0.9781534671783447, dice:0.9630782881173494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.96075489 0.97022786]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:8 , filename: /tmp/tmpasvy0u2w/pred_ZY_labels_180.h5, accuracy:0.9793840050697327, dice:0.9654913723731396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.95825765 0.96789893]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:9 , filename: /tmp/tmpasvy0u2w/pred_YX_labels_270.h5, accuracy:0.9781534671783447, dice:0.9630782881173494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.96615345 0.97214472]\n",
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:10 , filename: /tmp/tmpasvy0u2w/pred_ZX_labels_270.h5, accuracy:0.9822466373443604, dice:0.9691490848264754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dicescores = [0.96289314 0.97282006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:11 , filename: /tmp/tmpasvy0u2w/pred_ZY_labels_270.h5, accuracy:0.9805123209953308, dice:0.9678566006916457\n",
      "NN2 train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [00:00<00:00, 138719.61it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 221102.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup NN2 MLPClassifier\n",
      "NN2 MLPClassifier fit with 4096 samples, (y_train 4096 samples)\n",
      "Iteration 1, loss = 0.79793337\n",
      "Iteration 2, loss = 0.73763244\n",
      "Iteration 3, loss = 0.67106583\n",
      "Iteration 4, loss = 0.60795652\n",
      "Iteration 5, loss = 0.55015257\n",
      "Iteration 6, loss = 0.49913472\n",
      "Iteration 7, loss = 0.45497860\n",
      "Iteration 8, loss = 0.41691519\n",
      "Iteration 9, loss = 0.38458729\n",
      "Iteration 10, loss = 0.35661016\n",
      "Iteration 11, loss = 0.33268183\n",
      "Iteration 12, loss = 0.31191814\n",
      "Iteration 13, loss = 0.29375262\n",
      "Iteration 14, loss = 0.27809405\n",
      "Iteration 15, loss = 0.26397759\n",
      "Iteration 16, loss = 0.25163672\n",
      "Iteration 17, loss = 0.24054024\n",
      "Iteration 18, loss = 0.23048288\n",
      "Iteration 19, loss = 0.22148418\n",
      "Iteration 20, loss = 0.21312075\n",
      "Iteration 21, loss = 0.20554405\n",
      "Iteration 22, loss = 0.19853141\n",
      "Iteration 23, loss = 0.19211253\n",
      "Iteration 24, loss = 0.18600762\n",
      "Iteration 25, loss = 0.18042031\n",
      "Iteration 26, loss = 0.17522408\n",
      "Iteration 27, loss = 0.17031705\n",
      "Iteration 28, loss = 0.16570081\n",
      "Iteration 29, loss = 0.16137306\n",
      "Iteration 30, loss = 0.15723113\n",
      "Iteration 31, loss = 0.15335227\n",
      "Iteration 32, loss = 0.14969512\n",
      "Iteration 33, loss = 0.14622345\n",
      "Iteration 34, loss = 0.14284546\n",
      "Iteration 35, loss = 0.13971227\n",
      "Iteration 36, loss = 0.13670034\n",
      "Iteration 37, loss = 0.13385979\n",
      "Iteration 38, loss = 0.13111905\n",
      "Iteration 39, loss = 0.12850306\n",
      "Iteration 40, loss = 0.12600896\n",
      "Iteration 41, loss = 0.12367293\n",
      "Iteration 42, loss = 0.12137392\n",
      "Iteration 43, loss = 0.11925354\n",
      "Iteration 44, loss = 0.11714875\n",
      "Iteration 45, loss = 0.11519111\n",
      "Iteration 46, loss = 0.11330465\n",
      "Iteration 47, loss = 0.11147313\n",
      "Iteration 48, loss = 0.10977279\n",
      "Iteration 49, loss = 0.10808767\n",
      "Iteration 50, loss = 0.10651704\n",
      "Iteration 51, loss = 0.10494407\n",
      "Iteration 52, loss = 0.10349188\n",
      "Iteration 53, loss = 0.10207431\n",
      "Iteration 54, loss = 0.10071915\n",
      "Iteration 55, loss = 0.09941209\n",
      "Iteration 56, loss = 0.09817189\n",
      "Iteration 57, loss = 0.09696811\n",
      "Iteration 58, loss = 0.09581160\n",
      "Iteration 59, loss = 0.09471530\n",
      "Iteration 60, loss = 0.09362983\n",
      "Iteration 61, loss = 0.09261043\n",
      "Iteration 62, loss = 0.09160846\n",
      "Iteration 63, loss = 0.09066753\n",
      "Iteration 64, loss = 0.08972808\n",
      "Iteration 65, loss = 0.08884124\n",
      "Iteration 66, loss = 0.08796832\n",
      "Iteration 67, loss = 0.08714907\n",
      "Iteration 68, loss = 0.08637260\n",
      "Iteration 69, loss = 0.08560173\n",
      "Iteration 70, loss = 0.08485071\n",
      "Iteration 71, loss = 0.08415630\n",
      "Iteration 72, loss = 0.08345730\n",
      "Iteration 73, loss = 0.08277609\n",
      "Iteration 74, loss = 0.08213046\n",
      "Iteration 75, loss = 0.08150180\n",
      "Iteration 76, loss = 0.08089787\n",
      "Iteration 77, loss = 0.08033777\n",
      "Iteration 78, loss = 0.07975663\n",
      "Iteration 79, loss = 0.07922240\n",
      "Iteration 80, loss = 0.07867692\n",
      "Iteration 81, loss = 0.07814550\n",
      "Iteration 82, loss = 0.07764845\n",
      "Iteration 83, loss = 0.07717459\n",
      "Iteration 84, loss = 0.07670663\n",
      "Iteration 85, loss = 0.07624307\n",
      "Iteration 86, loss = 0.07580804\n",
      "Iteration 87, loss = 0.07537997\n",
      "Iteration 88, loss = 0.07496486\n",
      "Iteration 89, loss = 0.07455802\n",
      "Iteration 90, loss = 0.07415724\n",
      "Iteration 91, loss = 0.07376771\n",
      "Iteration 92, loss = 0.07339183\n",
      "Iteration 93, loss = 0.07305488\n",
      "Iteration 94, loss = 0.07269281\n",
      "Iteration 95, loss = 0.07236276\n",
      "Iteration 96, loss = 0.07207871\n",
      "Iteration 97, loss = 0.07169884\n",
      "Iteration 98, loss = 0.07139057\n",
      "Iteration 99, loss = 0.07107684\n",
      "Iteration 100, loss = 0.07077747\n",
      "Iteration 101, loss = 0.07050776\n",
      "Iteration 102, loss = 0.07021194\n",
      "Iteration 103, loss = 0.06993224\n",
      "Iteration 104, loss = 0.06966112\n",
      "Iteration 105, loss = 0.06939876\n",
      "Iteration 106, loss = 0.06914269\n",
      "Iteration 107, loss = 0.06887861\n",
      "Iteration 108, loss = 0.06868933\n",
      "Iteration 109, loss = 0.06839678\n",
      "Iteration 110, loss = 0.06816841\n",
      "Iteration 111, loss = 0.06795481\n",
      "Iteration 112, loss = 0.06771355\n",
      "Iteration 113, loss = 0.06751116\n",
      "Iteration 114, loss = 0.06732072\n",
      "Iteration 115, loss = 0.06706723\n",
      "Iteration 116, loss = 0.06686735\n",
      "Iteration 117, loss = 0.06665877\n",
      "Iteration 118, loss = 0.06648619\n",
      "Iteration 119, loss = 0.06627798\n",
      "Iteration 120, loss = 0.06609602\n",
      "Iteration 121, loss = 0.06590264\n",
      "Iteration 122, loss = 0.06573044\n",
      "Iteration 123, loss = 0.06555392\n",
      "Iteration 124, loss = 0.06539718\n",
      "Iteration 125, loss = 0.06524448\n",
      "Iteration 126, loss = 0.06503666\n",
      "Iteration 127, loss = 0.06488769\n",
      "Iteration 128, loss = 0.06472184\n",
      "Iteration 129, loss = 0.06456218\n",
      "Iteration 130, loss = 0.06440351\n",
      "Iteration 131, loss = 0.06428746\n",
      "Iteration 132, loss = 0.06411021\n",
      "Iteration 133, loss = 0.06396436\n",
      "Iteration 134, loss = 0.06382310\n",
      "Iteration 135, loss = 0.06368874\n",
      "Iteration 136, loss = 0.06354191\n",
      "Iteration 137, loss = 0.06345259\n",
      "Iteration 138, loss = 0.06326809\n",
      "Iteration 139, loss = 0.06315638\n",
      "Iteration 140, loss = 0.06301244\n",
      "Iteration 141, loss = 0.06290932\n",
      "Iteration 142, loss = 0.06276772\n",
      "Iteration 143, loss = 0.06263984\n",
      "Iteration 144, loss = 0.06253671\n",
      "Iteration 145, loss = 0.06241281\n",
      "Iteration 146, loss = 0.06230446\n",
      "Iteration 147, loss = 0.06217737\n",
      "Iteration 148, loss = 0.06209561\n",
      "Iteration 149, loss = 0.06197156\n",
      "Iteration 150, loss = 0.06185419\n",
      "Iteration 151, loss = 0.06177095\n",
      "Iteration 152, loss = 0.06165814\n",
      "Iteration 153, loss = 0.06156127\n",
      "Iteration 154, loss = 0.06144369\n",
      "Iteration 155, loss = 0.06135272\n",
      "Iteration 156, loss = 0.06123658\n",
      "Iteration 157, loss = 0.06116038\n",
      "Iteration 158, loss = 0.06106528\n",
      "Iteration 159, loss = 0.06097527\n",
      "Iteration 160, loss = 0.06086520\n",
      "Iteration 161, loss = 0.06076755\n",
      "Iteration 162, loss = 0.06074274\n",
      "Iteration 163, loss = 0.06061141\n",
      "Iteration 164, loss = 0.06051739\n",
      "Iteration 165, loss = 0.06043082\n",
      "Iteration 166, loss = 0.06036829\n",
      "Iteration 167, loss = 0.06024608\n",
      "Iteration 168, loss = 0.06016405\n",
      "Iteration 169, loss = 0.06009160\n",
      "Iteration 170, loss = 0.06005275\n",
      "Iteration 171, loss = 0.05995055\n",
      "Iteration 172, loss = 0.05986445\n",
      "Iteration 173, loss = 0.05977377\n",
      "Iteration 174, loss = 0.05974290\n",
      "Iteration 175, loss = 0.05961779\n",
      "Iteration 176, loss = 0.05955028\n",
      "Iteration 177, loss = 0.05947263\n",
      "Iteration 178, loss = 0.05943310\n",
      "Iteration 179, loss = 0.05933362\n",
      "Iteration 180, loss = 0.05926724\n",
      "Iteration 181, loss = 0.05918696\n",
      "Iteration 182, loss = 0.05916814\n",
      "Iteration 183, loss = 0.05907608\n",
      "Iteration 184, loss = 0.05900559\n",
      "Iteration 185, loss = 0.05896593\n",
      "Iteration 186, loss = 0.05892812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "NN2 train score:0.981201171875\n",
      "Preparing to predict the whole training volume\n",
      "NN2_predict()\n",
      "Data type is numpy.ndarray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:MetricScoreOfVols_Accuracy()\n",
      "INFO:root:MetricScoreOfVols_Dice()\n",
      "INFO:root:Number of class segmentations in first volume 3\n",
      "INFO:root:Number of class segmentations in second volume 3\n",
      "INFO:root:dicescores = [0.97052838 0.98127944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN2 acc:0.9845385551452637, dice:0.9759039102170555\n",
      "Segmentor training complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Run segmentor train()\")\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#Run training\n",
    "lgsegmentor0.train(traindata, trainlabels, get_metrics=True)\n",
    "\n",
    "print(\"Segmentor training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of large volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import leopardgecko.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict a whole (large) volume\n",
      "Loading file: /ceph/users/ypu66991/data/GasHydrate/89062_1554x1554x200_uint8_data_clipped.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Predict a whole (large) volume\")\n",
    "\n",
    "to_pred_fn=\"/ceph/users/ypu66991/data/GasHydrate/89062_1554x1554x200_uint8_data_clipped.h5\"\n",
    "print(f\"Loading file: {to_pred_fn}\")\n",
    "data_to_pred = leopardgecko.utils.numpy_from_hdf5(to_pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model dictionary from file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction starting\n",
      "NN1 prediction\n",
      "Created temporary folder /tmp/tmp77pwcrvt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sending the U-Net model to device 0\n",
      "INFO:root:Loading in the saved weights.\n",
      "INFO:root:Volume rotated by 0 degrees\n",
      "INFO:root:Predicting YX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (2000, 1554, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 500/500 [03:45<00:00,  2.22it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp77pwcrvt/pred_YX_0.h5.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPrediction starting\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m data_pred \u001b[39m=\u001b[39m lgsegmentor0\u001b[39m.\u001b[39;49mpredict(data_to_pred)\n",
      "File \u001b[0;32m/ceph/users/ypu66991/Programming/Git_LeopardGecko/leopardgecko/segmentor.py:168\u001b[0m, in \u001b[0;36mcMultiAxisRotationsSegmentor.predict\u001b[0;34m(self, data_in)\u001b[0m\n\u001b[1;32m    166\u001b[0m tempdir_pred_path \u001b[39m=\u001b[39m Path(tempdir_pred\u001b[39m.\u001b[39mname)\n\u001b[1;32m    167\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCreated temporary folder \u001b[39m\u001b[39m{\u001b[39;00mtempdir_pred_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 168\u001b[0m pred_data_probs_filenames, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mNN1_predict(data_in, tempdir_pred_path) \u001b[39m#Get prediction probs, not labels\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNN1 prediction, complete.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[39m#Build data object containing all predictions\u001b[39;00m\n",
      "File \u001b[0;32m/ceph/users/ypu66991/Programming/Git_LeopardGecko/leopardgecko/segmentor.py:414\u001b[0m, in \u001b[0;36mcMultiAxisRotationsSegmentor.NN1_predict\u001b[0;34m(self, data_to_predict, pred_folder_out)\u001b[0m\n\u001b[1;32m    412\u001b[0m pred_probs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrot90(res[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39mkrot) \u001b[39m#invert rotation before saving\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39m#Saves prediction labels\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m fn \u001b[39m=\u001b[39m save_pred_data(pred_probs, \u001b[39m\"\u001b[39;49m\u001b[39mYX\u001b[39;49m\u001b[39m\"\u001b[39;49m, rot_angle_degrees)\n\u001b[1;32m    415\u001b[0m pred_data_probs_filenames\u001b[39m.\u001b[39mappend(fn)\n\u001b[1;32m    417\u001b[0m pred_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrot90(res[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39mkrot)\n",
      "File \u001b[0;32m/ceph/users/ypu66991/Programming/Git_LeopardGecko/leopardgecko/segmentor.py:392\u001b[0m, in \u001b[0;36mcMultiAxisRotationsSegmentor.NN1_predict.<locals>.save_pred_data\u001b[0;34m(data, axis, rot)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_pred_data\u001b[39m(data, axis, rot):\n\u001b[1;32m    390\u001b[0m     \u001b[39m# Saves predicted data to h5 file in tempdir and return file path in case it is needed\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpred_folder_out\u001b[39m}\u001b[39;00m\u001b[39m/pred_\u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mrot\u001b[39m}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 392\u001b[0m     save_data_to_hdf5(data, file_path)\n\u001b[1;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m file_path\n",
      "File \u001b[0;32m/ceph/users/ypu66991/Programming/Git_LeopardGecko/leopardgecko/utils.py:8\u001b[0m, in \u001b[0;36msave_data_to_hdf5\u001b[0;34m(data, file_path, internal_path, chunking)\u001b[0m\n\u001b[1;32m      6\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSaving data of shape \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(file_path, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 8\u001b[0m     f\u001b[39m.\u001b[39;49mcreate_dataset(\n\u001b[1;32m      9\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/data\u001b[39;49m\u001b[39m\"\u001b[39;49m, data\u001b[39m=\u001b[39;49mdata, chunks\u001b[39m=\u001b[39;49mchunking, compression\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgzip\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m     )\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mrsplit(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmake_new_dset(group, shape, dtype, data, name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    184\u001b[0m dset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/h5py/_hl/dataset.py:48\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Empty):\n\u001b[1;32m     47\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m base\n\u001b[0;32m---> 48\u001b[0m     data \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39;49marray_for_new_object(data, specified_dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m     50\u001b[0m \u001b[39m# Validate shape\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/h5py/_hl/base.py:118\u001b[0m, in \u001b[0;36marray_for_new_object\u001b[0;34m(data, specified_dtype)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     as_dtype \u001b[39m=\u001b[39m guess_dtype(data)\n\u001b[0;32m--> 118\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(data, order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mas_dtype)\n\u001b[1;32m    120\u001b[0m \u001b[39m# In most cases, this does nothing. But if data was already an array,\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m# and as_dtype is a tagged h5py dtype (e.g. for an object array of strings),\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39m# asarray() doesn't replace its dtype object. This gives it the tagged dtype:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m as_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(\"Prediction starting\")\n",
    "\n",
    "# data_pred = lgsegmentor0.predict(data_to_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break up cMultiAxisRotationsSegmentor for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ceph/users/ypu66991/Programming/Git_LeopardGecko/scripts/lg_segmentor_model_NN1.pytorch\n",
      "MLPClassifier(activation='tanh', hidden_layer_sizes=(10, 10), max_iter=1000,\n",
      "              random_state=1, solver='sgd', verbose=True)\n"
     ]
    }
   ],
   "source": [
    "#Make sure they are ok\n",
    "print(lgsegmentor0.model_NN1_path)\n",
    "print(lgsegmentor0.NN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary folder /tmp/tmp67v577t0\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "tempdir_pred= tempfile.TemporaryDirectory()\n",
    "tempdir_pred_path = Path(tempdir_pred.name)\n",
    "print(f\"Created temporary folder {tempdir_pred_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model dictionary from file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN1 prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sending the U-Net model to device 0\n",
      "INFO:root:Loading in the saved weights.\n",
      "INFO:root:Volume rotated by 0 degrees\n",
      "INFO:root:Predicting YX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (2000, 1554, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 500/500 [03:58<00:00,  2.10it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_YX_0.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_YX_labels_0.h5.\n",
      "INFO:root:Predicting ZX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (1554, 2000, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 389/389 [03:56<00:00,  1.64it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_ZX_0.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_ZX_labels_0.h5.\n",
      "INFO:root:Predicting ZY slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (1554, 1554, 2000).\n",
      "Prediction batch: 100%|██████████████████████████████| 389/389 [04:33<00:00,  1.42it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_ZY_0.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_ZY_labels_0.h5.\n",
      "INFO:root:Volume rotated by 90 degrees\n",
      "INFO:root:Predicting YX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (1554, 2000, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 389/389 [03:43<00:00,  1.74it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_YX_90.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_YX_labels_90.h5.\n",
      "INFO:root:Predicting ZX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (2000, 1554, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 500/500 [03:35<00:00,  2.32it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_ZX_90.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_ZX_labels_90.h5.\n",
      "INFO:root:Predicting ZY slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (1554, 2000, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 389/389 [04:35<00:00,  1.41it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_ZY_90.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_ZY_labels_90.h5.\n",
      "INFO:root:Volume rotated by 180 degrees\n",
      "INFO:root:Predicting YX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (2000, 1554, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 500/500 [03:48<00:00,  2.18it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_YX_180.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_YX_labels_180.h5.\n",
      "INFO:root:Predicting ZX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (1554, 2000, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 389/389 [03:45<00:00,  1.72it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_ZX_180.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_ZX_labels_180.h5.\n",
      "INFO:root:Predicting ZY slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (1554, 1554, 2000).\n",
      "Prediction batch: 100%|██████████████████████████████| 389/389 [04:23<00:00,  1.47it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_ZY_180.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_ZY_labels_180.h5.\n",
      "INFO:root:Volume rotated by 270 degrees\n",
      "INFO:root:Predicting YX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (1554, 2000, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 389/389 [03:46<00:00,  1.72it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_YX_270.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_YX_labels_270.h5.\n",
      "INFO:root:Predicting ZX slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (2000, 1554, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 500/500 [03:47<00:00,  2.20it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_ZX_270.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_ZX_labels_270.h5.\n",
      "INFO:root:Predicting ZY slices:\n",
      "INFO:root:Free GPU memory is 31.28 GB. Batch size will be 4.\n",
      "INFO:root:Predicting segmentation for volume of shape (1554, 2000, 1554).\n",
      "Prediction batch: 100%|██████████████████████████████| 389/389 [04:27<00:00,  1.46it/s]\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554, 3) to /tmp/tmp67v577t0/pred_ZY_270.h5.\n",
      "INFO:root:Saving data of shape (2000, 1554, 1554) to /tmp/tmp67v577t0/pred_ZY_labels_270.h5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN1 prediction, complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"NN1 prediction.\")\n",
    "pred_data_probs_filenames, _ = lgsegmentor0.NN1_predict(data_to_pred, tempdir_pred_path) #Get prediction probs, not labels\n",
    "print(\"NN1 prediction, complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/tmp67v577t0/pred_YX_0.h5',\n",
       " '/tmp/tmp67v577t0/pred_ZX_0.h5',\n",
       " '/tmp/tmp67v577t0/pred_ZY_0.h5',\n",
       " '/tmp/tmp67v577t0/pred_YX_90.h5',\n",
       " '/tmp/tmp67v577t0/pred_ZX_90.h5',\n",
       " '/tmp/tmp67v577t0/pred_ZY_90.h5',\n",
       " '/tmp/tmp67v577t0/pred_YX_180.h5',\n",
       " '/tmp/tmp67v577t0/pred_ZX_180.h5',\n",
       " '/tmp/tmp67v577t0/pred_ZY_180.h5',\n",
       " '/tmp/tmp67v577t0/pred_YX_270.h5',\n",
       " '/tmp/tmp67v577t0/pred_ZX_270.h5',\n",
       " '/tmp/tmp67v577t0/pred_ZY_270.h5']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data_probs_filenames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in files, with filenames in list `pred_data_probs_filenames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'leopardgecko.utils' from '/ceph/users/ypu66991/Programming/Git_LeopardGecko/leopardgecko/utils.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(leopardgecko.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data0 shape:(2000, 1554, 1554, 3), dtype:float16\n"
     ]
    }
   ],
   "source": [
    "#Build data object containing all predictions\n",
    "import dask.array as da\n",
    "data0 = leopardgecko.utils.read_h5_to_da(pred_data_probs_filenames[0])\n",
    "#data0 = leopardgecko.utils.read_h5_to_np(pred_data_probs_filenames[0])\n",
    "print(f\"data0 shape:{data0.shape}, dtype:{data0.dtype}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading gzip h5 file is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_shape:(12, 2000, 1554, 1554, 3)\n"
     ]
    }
   ],
   "source": [
    "all_shape = ( len(pred_data_probs_filenames), *data0.shape )\n",
    "print(f\"all_shape:{all_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks_shape:(12, 128, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "#Prepare accumulation array (dask)\n",
    "chunks_shape = (len(pred_data_probs_filenames), 128,128,128, data0.shape[-1] )\n",
    "print(f\"chunks_shape:{chunks_shape}\")\n",
    "data_all=da.zeros(all_shape, chunks=chunks_shape, dtype=data0.dtype)\n",
    "#in case of 12 predictions and 3 labels, the chunks will be (12,128,128,128,3) size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill with data\n",
    "data_all[0,:,:,:]= data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file number:1\n",
      "Read file number:2\n",
      "Read file number:3\n",
      "Read file number:4\n",
      "Read file number:5\n",
      "Read file number:6\n",
      "Read file number:7\n",
      "Read file number:8\n",
      "Read file number:9\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1,len(pred_data_probs_filenames)):\n",
    "    print(f\"Read file number:{i}\")\n",
    "    data_i = leopardgecko.utils.read_h5_to_da(pred_data_probs_filenames[i])\n",
    "    #data_i = leopardgecko.utils.read_h5_to_np(pred_data_probs_filenames[i])\n",
    "    data_all[i,:,:,:]=data_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float16')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it can access data to do calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_all\u001b[39m.\u001b[39;49mmax()\u001b[39m.\u001b[39;49mcompute()\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_all.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ceph/users/ypu66991/Programming/Git_LeopardGecko/scripts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'leopardgecko.segmentor' from '/ceph/users/ypu66991/Programming/Git_LeopardGecko/leopardgecko/segmentor.py'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN2 prediction\n",
      "NN2_predict()\n",
      "Data type is dask.core.Array\n",
      "Starting dask computation\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dset_id is not a dataset id (dset_id is not a dataset ID)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNN2 prediction\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m d_prediction\u001b[39m=\u001b[39m lgsegmentor0\u001b[39m.\u001b[39;49mNN2_predict( data_all)\n",
      "File \u001b[0;32m/ceph/users/ypu66991/Programming/Git_LeopardGecko/leopardgecko/segmentor.py:313\u001b[0m, in \u001b[0;36mcMultiAxisRotationsSegmentor.NN2_predict\u001b[0;34m(self, data_all_probs)\u001b[0m\n\u001b[1;32m    305\u001b[0m b \u001b[39m=\u001b[39m da\u001b[39m.\u001b[39mreduction(data_all_probs,\n\u001b[1;32m    306\u001b[0m                 chunk\u001b[39m=\u001b[39mchunkf,\n\u001b[1;32m    307\u001b[0m                 aggregate\u001b[39m=\u001b[39m aggf,\n\u001b[1;32m    308\u001b[0m                 dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels_dtype,\n\u001b[1;32m    309\u001b[0m                 keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m                 axis\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m4\u001b[39m)) \u001b[39m#It appeears that his axis parameter is simply passed to chnkf and aggf and that's it.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting dask computation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 313\u001b[0m b_comp\u001b[39m=\u001b[39mb\u001b[39m.\u001b[39mcompute()\n\u001b[1;32m    314\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCompleted. res shape:\u001b[39m\u001b[39m{\u001b[39;00mb_comp\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    316\u001b[0m \u001b[39mreturn\u001b[39;00m b_comp\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[1;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/dask/array/core.py:120\u001b[0m, in \u001b[0;36mgetter\u001b[0;34m(a, b, asarray, lock)\u001b[0m\n\u001b[1;32m    118\u001b[0m     lock\u001b[39m.\u001b[39macquire()\n\u001b[1;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     c \u001b[39m=\u001b[39m a[b]\n\u001b[1;32m    121\u001b[0m     \u001b[39m# Below we special-case `np.matrix` to force a conversion to\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39m# `np.ndarray` and preserve original Dask behavior for `getter`,\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[39m# as for all purposes `np.matrix` is array-like and thus\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[39m# `is_arraylike` evaluates to `True` in that case.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[39mif\u001b[39;00m asarray \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m is_arraylike(c) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(c, np\u001b[39m.\u001b[39mmatrix)):\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/ceph/users/ypu66991/conda_envs/p9dev/lib/python3.9/site-packages/h5py/_hl/dataset.py:768\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fast_read_ok \u001b[39mand\u001b[39;00m (new_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    767\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fast_reader\u001b[39m.\u001b[39;49mread(args)\n\u001b[1;32m    769\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall back to Python read pathway below\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_selector.pyx:376\u001b[0m, in \u001b[0;36mh5py._selector.Reader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dset_id is not a dataset id (dset_id is not a dataset ID)"
     ]
    }
   ],
   "source": [
    "print(\"NN2 prediction\")\n",
    "d_prediction= lgsegmentor0.NN2_predict( data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save result \n",
    "leopardgecko.utils.save_data_to_hdf5(d_prediction, \"89062_1554x1554x200_uint8_data_clipped_lg_segm_pred.h5\")\n",
    "print(\"Save Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"NN2 prediction complete.\")\n",
    "print(f\"Cleaning up tempdir_pred: {tempdir_pred_path}\")\n",
    "tempdir_pred.cleanup()\n",
    "\n",
    "return d_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
